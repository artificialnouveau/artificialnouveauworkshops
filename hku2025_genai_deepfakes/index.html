<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>Generative AI &amp; Deepfakes — HKU 2026</title>
  <style>
    *, *::before, *::after { box-sizing: border-box; margin: 0; padding: 0; }

    :root {
      --bg: #0a0a0a;
      --bg-card: #141414;
      --text: #e8e8e8;
      --dim: #777;
      --accent: #4a9eff;
      --green: #00ff66;
      --red: #ff4a4a;
      --yellow: #ffd94a;
      --pink: #ff6ec7;
      --orange: #ff8c42;
      --mono: 'SF Mono', 'Fira Code', 'Consolas', monospace;
      --sans: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, sans-serif;
    }

    body {
      background: var(--bg);
      color: var(--text);
      font-family: var(--sans);
      overflow: hidden;
      height: 100vh;
      -webkit-font-smoothing: antialiased;
    }

    .slide {
      display: none;
      position: absolute;
      inset: 0;
      padding: 40px 80px;
      flex-direction: column;
      align-items: center;
      text-align: center;
      overflow-y: auto;
    }

    .slide::before, .slide::after { content: ''; flex: 1 0 0px; }
    .slide > *:first-child { margin-top: 0; }
    .slide.active { display: flex; animation: fadeIn 0.35s ease; }
    .slide.fade-out { display: flex; animation: fadeOut 0.25s ease forwards; }

    @keyframes fadeIn { from { opacity: 0; } to { opacity: 1; } }
    @keyframes fadeOut { from { opacity: 1; } to { opacity: 0; } }

    .fragment { opacity: 0; transition: opacity 0.4s ease; }
    .fragment.visible { opacity: 1; }

    .slide h1 { font-size: 4.2rem; font-weight: 700; letter-spacing: -0.03em; line-height: 1.15; margin-bottom: 16px; }
    .slide h2 { font-size: 3.2rem; font-weight: 600; letter-spacing: -0.02em; line-height: 1.2; margin-bottom: 20px; }
    .slide h3 { font-size: 1.8rem; font-weight: 600; margin-bottom: 12px; color: var(--accent); }
    .slide p, .slide li { font-size: 1.5rem; line-height: 1.7; color: var(--dim); }
    .slide ul { list-style: none; padding: 0; text-align: left; }
    .slide ul li { padding: 6px 0; padding-left: 24px; position: relative; }
    .slide ul li::before { content: '\2192'; position: absolute; left: 0; color: var(--accent); font-family: var(--mono); }
    .slide strong { color: var(--text); font-weight: 600; }
    .slide em { color: var(--accent); font-style: normal; }
    .slide a { color: var(--accent); text-decoration: none; }
    .slide a:hover { text-decoration: underline; }

    .accent { color: var(--accent); }
    .green { color: var(--green); }
    .red { color: var(--red); }
    .yellow { color: var(--yellow); }
    .pink { color: var(--pink); }
    .orange { color: var(--orange); }

    .title-slide { align-items: center; text-align: center; justify-content: center; }
    .title-slide h1 { font-size: 4.8rem; }
    .title-slide .subtitle { font-size: 1.6rem; color: var(--dim); font-family: var(--mono); margin-top: 12px; }

    .section-title { align-items: center; text-align: center; justify-content: center; }
    .section-title h2 { font-size: 3.8rem; color: var(--accent); }
    .section-title .divider { width: 80px; height: 3px; background: var(--accent); margin: 20px auto; border-radius: 2px; }

    .sub-section { align-items: center; text-align: center; justify-content: center; }
    .sub-section h3 { font-size: 2.4rem; color: var(--dim); letter-spacing: 0.05em; text-transform: uppercase; }
    .sub-section .sub-divider { width: 40px; height: 2px; background: var(--dim); margin: 12px auto; border-radius: 2px; }

    .two-col { display: grid; grid-template-columns: 1fr 1fr; gap: 20px; align-items: center; width: 100%; text-align: left; }
    .three-col { display: grid; grid-template-columns: 1fr 1fr 1fr; gap: 16px; align-items: start; width: 100%; text-align: left; }

    .img-row { display: flex; gap: 16px; align-items: center; justify-content: center; flex-wrap: wrap; width: 100%; }
    .img-row img { max-height: 320px; border-radius: 8px; object-fit: contain; }

    .card { background: var(--bg-card); border-radius: 12px; padding: 24px; border-left: 3px solid var(--accent); text-align: left; }
    .card.green-border { border-left-color: var(--green); }
    .card.red-border { border-left-color: var(--red); }
    .card.yellow-border { border-left-color: var(--yellow); }
    .card.pink-border { border-left-color: var(--pink); }
    .card.orange-border { border-left-color: var(--orange); }

    .slide img { max-width: 100%; max-height: 70vh; border-radius: 8px; object-fit: contain; width: auto; height: auto; }
    .slide img.full { max-height: 85vh; margin: 0 auto; display: block; }
    .slide img.small { max-height: 220px; }
    .slide img.medium { max-height: 400px; }
    .img-row img { max-height: 400px; object-fit: contain; }
    .card img { max-width: 100%; height: auto; object-fit: contain; border-radius: 6px; }
    .slide video, .slide iframe { border-radius: 8px; }

    .caption { font-size: 0.95rem; color: var(--dim); font-family: var(--mono); text-align: center; margin-top: 8px; }

    .quote { font-size: 1.8rem; font-style: italic; line-height: 1.6; color: var(--text); border-left: 4px solid var(--accent); padding: 20px 30px; background: var(--bg-card); border-radius: 0 12px 12px 0; max-width: 700px; text-align: left; }

    .warning-badge { display: inline-block; padding: 10px 24px; border: 2px solid var(--red); border-radius: 8px; color: var(--red); font-family: var(--mono); font-size: 1.1rem; margin-top: 16px; }

    table.data-table {
      width: 100%;
      border-collapse: collapse;
      margin-top: 16px;
      font-size: 1.15rem;
    }
    table.data-table th, table.data-table td {
      padding: 16px 20px;
      text-align: left;
      border-bottom: 1px solid #222;
    }
    table.data-table th {
      color: var(--accent);
      font-weight: 600;
      font-size: 1rem;
      text-transform: uppercase;
      letter-spacing: 0.05em;
    }
    table.data-table td { color: var(--dim); }
    table.data-table td:first-child { color: var(--text); font-weight: 500; }
    table.data-table tr:hover td { color: var(--text); }

    .tool-grid { display: grid; grid-template-columns: 1fr 1fr; gap: 12px; width: 100%; }

    #nav { position: fixed; bottom: 24px; right: 30px; display: flex; gap: 8px; z-index: 100; }
    #nav button { width: 40px; height: 40px; border: 1px solid #333; background: var(--bg-card); color: var(--text); border-radius: 8px; cursor: pointer; font-size: 1rem; transition: all 0.2s; }
    #nav button:hover { border-color: var(--accent); color: var(--accent); }
    #slide-counter { position: fixed; bottom: 30px; left: 30px; font-family: var(--mono); font-size: 0.8rem; color: var(--dim); z-index: 100; }
    #progress { position: fixed; top: 0; left: 0; height: 3px; background: var(--accent); transition: width 0.3s ease; z-index: 100; }

    #section-nav {
      position: fixed; top: 3px; left: 0; right: 0; z-index: 99;
      display: flex; justify-content: center; gap: 4px; padding: 8px 16px;
      background: linear-gradient(180deg, rgba(10,10,10,0.95) 0%, rgba(10,10,10,0.8) 80%, transparent 100%);
      opacity: 0; pointer-events: none; transition: opacity 0.3s ease;
      flex-wrap: wrap;
    }
    #section-nav.visible { opacity: 1; pointer-events: auto; }
    #section-nav button {
      border: 1px solid #333; background: var(--bg-card); color: var(--dim);
      border-radius: 20px; padding: 4px 14px; cursor: pointer;
      font-family: var(--mono); font-size: 0.7rem; white-space: nowrap;
      transition: all 0.2s;
    }
    #section-nav button:hover { border-color: var(--accent); color: var(--accent); }
    #section-nav button.active { border-color: var(--accent); color: var(--accent); background: rgba(74,158,255,0.12); }

    @media (max-width: 900px) {
      .slide { padding: 30px 24px; }
      .slide h1 { font-size: 2.2rem; }
      .title-slide h1 { font-size: 2.6rem; }
      .slide h2 { font-size: 1.8rem; }
      .two-col, .three-col, .tool-grid { grid-template-columns: 1fr; gap: 20px; }
      .quote { font-size: 1.2rem; }
    }
  </style>
</head>
<body>

  <div id="progress"></div>
  <div id="section-nav"></div>

  <!-- ============================================ -->
  <!-- PART 1: INTRODUCTION                         -->
  <!-- ============================================ -->

  <!-- Title -->
  <div class="slide title-slide active" data-notes="Welcome everyone to Part 2 of the HKU workshop. Today we cover Generative AI and Deepfakes. Press P to open presenter notes.">
    <h1><span class="accent">Generative AI</span> &amp;<br>Deepfakes</h1>
    <p class="subtitle">HKU 2026 Workshop — Part 2</p>
    <p class="subtitle">Ahnjili ZhuParris</p>
  </div>

  <!-- Outline -->
  <div class="slide" data-notes="Here's the roadmap. We start with what generative AI actually is, then examine it through five characteristics of modern AI empires — drawing on Karen Hao's Empire of AI framework. After the conclusion we move into deepfakes and then the hands-on workshop with tools and resources.">
    <h2>Outline</h2>
    <div class="two-col" style="font-size:1.4rem; max-width:800px; gap:24px">
      <ul>
        <li>What is Generative AI</li>
        <li style="color:var(--red)">1. Resource Appropriation</li>
        <li style="color:var(--green)">2. Scale at All Costs</li>
        <li style="color:var(--orange)">3. Labor Exploitation</li>
      </ul>
      <ul>
        <li style="color:var(--pink)">4. The Civilizing Mission</li>
        <li style="color:var(--accent)">5. Competitive Justification</li>
        <li>Conclusion</li>
        <li style="color:var(--dim)">Deepfakes &amp; Consent</li>
        <li style="color:var(--green)">Workshop + Tools</li>
      </ul>
    </div>
  </div>

  <!-- About Me -->
  <div class="slide" data-notes="Brief intro — I work as an AI engineer specialising in computer vision, and I'm also an AI artist exploring the creative side of these technologies.">
    <h2>About Me</h2>
    <div style="display:flex; gap:24px; align-items:center; justify-content:center; margin-bottom:24px">
      <div class="card" style="padding:16px 28px"><h3 style="margin:0">AI Engineer</h3></div>
      <div class="card pink-border" style="padding:16px 28px"><h3 style="margin:0; color:var(--pink)">AI Artist</h3></div>
    </div>
    <div class="img-row">
      <img src="img/slide2_Picture_9.jpg" class="small">
      <img src="img/slide2_Picture_11.jpg" class="small">
      <img src="img/slide2_Picture_13.jpg" class="small">
      <img src="img/slide2_Picture_15.jpg" class="small">
    </div>
  </div>

  <div class="slide" data-notes="This framework comes from Karen Hao's book Empire of AI (2025). Hao spent years reporting on OpenAI from the inside and argues that AI companies aren't just tech companies — they're modern empires. The key insight: the logic driving AI expansion isn't new. It's colonial logic — the same patterns used by the East India Company, the Belgian Congo, and the British Empire. Hao identifies characteristics that map directly onto how empires have always operated. Resource Appropriation — empires laid claim to land, minerals, and labor that weren't theirs. AI companies do the same with data, creative work, and natural resources. They scraped the entire internet without permission — billions of images, books, articles, code — to build proprietary systems worth trillions. Labor Exploitation — empires ran on cheap and coerced labor. AI runs on $2/hour content moderators in Kenya and the Philippines who review traumatic content so the models can be 'safe'. The wealth flows to Silicon Valley; the human cost is borne by the Global South. Competitive Justification — every empire justified expansion by pointing to a rival. 'If we don't colonize Africa, France will.' Today: 'We must build AGI before China does.' This framing makes safety concerns sound like weakness and turns reckless speed into patriotic duty. The Civilizing Mission — empires claimed to bring progress and enlightenment to 'backward' peoples. AI companies claim to 'democratize AI' and 'benefit all of humanity' — while training on Western data, encoding Western values, and flattening the world's cultural diversity into a single homogenized output. Scale at All Costs — empires needed constant expansion to survive — more territory, more resources, more subjects. AI needs the same: more data centres, more water for cooling, more energy, more land. Microsoft is restarting Three Mile Island. Google's emissions rose 48% in one year. This is imperial expansion in digital form. Supporting references: Kate Crawford, Atlas of AI (2021) — the material costs of AI. Nick Couldry and Ulises Mejias, The Costs of Connection (2019) — data colonialism as the new extractive frontier.">
    <h2>The Empire of AI</h2>
    <p style="color:var(--dim); margin-bottom:20px; font-size:1.2rem">Five characteristics of modern AI empires</p>
    <div style="display:flex; gap:24px; align-items:start; max-width:1100px; width:100%">
      <img src="img/empire_of_ai_cover.jpg" style="width:180px; border-radius:8px; flex-shrink:0; box-shadow:0 4px 20px rgba(0,0,0,0.4)">
      <div style="display:grid; grid-template-columns:1fr 1fr; gap:12px; flex:1">
        <div class="card red-border" style="padding:14px 18px">
          <h3 style="color:var(--red); font-size:1.1rem">1. Resource Appropriation</h3>
          <p style="font-size:1rem">Laying claim to data, creative work, and natural resources</p>
        </div>
        <div class="card orange-border fragment" style="padding:14px 18px">
          <h3 style="color:var(--orange); font-size:1.1rem">2. Labor Exploitation</h3>
          <p style="font-size:1rem">$2/hour content moderators while Silicon Valley earns billions</p>
        </div>
        <div class="card yellow-border fragment" style="padding:14px 18px">
          <h3 style="color:var(--yellow); font-size:1.1rem">3. Competitive Justification</h3>
          <p style="font-size:1rem">"We must build AGI before China does" — good empire vs evil empire</p>
        </div>
        <div class="card pink-border fragment" style="padding:14px 18px">
          <h3 style="color:var(--pink); font-size:1.1rem">4. The Civilizing Mission</h3>
          <p style="font-size:1rem">"Democratizing AI" while extracting and homogenizing</p>
        </div>
        <div class="card green-border fragment" style="padding:14px 18px; grid-column: span 2">
          <h3 style="color:var(--green); font-size:1.1rem">5. Scale at All Costs</h3>
          <p style="font-size:1rem">Imperial expansion demanding more land, water, and energy for data centres</p>
        </div>
      </div>
    </div>
  </div>

  <!-- ============================================ -->
  <!-- PART 2: WHAT IS GENERATIVE AI                -->
  <!-- ============================================ -->

  <div class="slide section-title" data-notes="Let's start with the basics. What exactly do we mean by generative AI?">
    <h2>What is <span class="accent">Generative AI</span></h2>
    <div class="divider"></div>
  </div>

  <div class="slide" data-notes="Generative AI is about creating new content — not just classifying or analysing existing data. The key inputs and outputs span text, images, sound, animation, and 3D models. The crucial insight: these models learn statistical patterns and produce novel outputs that resemble their training data.">
    <h2>Generative AI</h2>
    <div class="card" style="max-width:750px; margin-bottom:24px">
      <p style="font-size:1.5rem; color:var(--text); line-height:1.8">
        <em>Generative AI</em> enables users to quickly generate <strong>synthetic media</strong> based on a variety of inputs.
      </p>
    </div>
    <div class="three-col fragment" style="max-width:800px">
      <div class="card"><h3>Inputs &amp; Outputs</h3>
        <ul>
          <li>Text</li>
          <li>Images</li>
          <li>Sounds</li>
          <li>Animation</li>
          <li>3D Models</li>
        </ul>
      </div>
      <div class="card green-border" style="grid-column: span 2">
        <h3 style="color:var(--green)">Key Idea</h3>
        <p style="color:var(--text)">Models learn patterns from training data and generate <strong>new content</strong> that resembles — but is not identical to — what they were trained on.</p>
      </div>
    </div>
  </div>

  <div class="slide" data-notes="Training a generative model requires massive datasets and significant compute. These images show the training pipeline — from raw data collection through to the neural network learning patterns over many iterations.">
    <h2>Training Time</h2>
    <div style="display:grid; grid-template-columns:1fr 1fr 1fr; grid-template-rows:1fr 1fr; gap:12px; max-width:1100px; width:100%">
      <img src="img/slide5_Picture_8.jpg" style="grid-column:1; grid-row:1; width:100%; border-radius:8px; object-fit:contain">
      <img src="img/slide5_Picture_12.png" class="fragment" style="grid-column:1; grid-row:2; width:100%; border-radius:8px; object-fit:contain">
      <img src="img/slide5_Picture_2.jpg" class="fragment" style="grid-column:2; grid-row:1; width:100%; border-radius:8px; object-fit:contain">
      <img src="img/slide5_Picture_4.png" class="fragment" style="grid-column:2; grid-row:2; width:100%; border-radius:8px; object-fit:contain">
      <img src="img/slide5_Picture_6.jpg" class="fragment" style="grid-column:3; grid-row:1; width:100%; border-radius:8px; object-fit:contain">
      <img src="img/slide5_Picture_10.jpg" class="fragment" style="grid-column:3; grid-row:2; width:100%; border-radius:8px; object-fit:contain">
    </div>
  </div>

  <div class="slide" data-notes="Two primary methods for controlling model behaviour. First: update the training data — more diverse and higher quality data leads to better outputs. Second: reinforcement learning from human feedback (RLHF) — humans rate outputs and the model adjusts. This is how ChatGPT was fine-tuned.">
    <h2>How to Control a Model</h2>
    <div class="two-col" style="max-width:900px">
      <div class="card yellow-border">
        <h3 style="color:var(--yellow)">Update the Training Dataset</h3>
        <ul>
          <li>Increase the <strong>diversity</strong> of the dataset</li>
          <li>Improve the <strong>quality</strong> of the data</li>
        </ul>
      </div>
      <div class="card green-border fragment">
        <h3 style="color:var(--green)">Reinforcement Learning</h3>
        <ul>
          <li>Provide <strong>user feedback</strong></li>
          <li>Human-in-the-loop training</li>
        </ul>
      </div>
    </div>
    <div class="img-row fragment" style="margin-top:20px">
      <img src="img/slide6_Picture_16.jpg" class="medium">
      <img src="img/slide6_Picture_19.jpg" class="medium">
    </div>
  </div>

  <div class="slide" data-notes="Two more control mechanisms. Limiting output: reducing the weights of certain terms — for example, making it harder to generate violent or racist imagery. Prompt transformation: the system rewrites user prompts before they reach the model, adding safety constraints or expanding vague requests. Ask students: which approach do they think is more effective?">
    <h2>How to Control a Model</h2>
    <div class="two-col" style="max-width:900px; align-items:start">
      <div class="card red-border">
        <h3 style="color:var(--red)">Limit What It Can Produce</h3>
        <p>Reduce the weights of certain terms or content (such as violent or racist imagery)</p>
        <img src="img/slide7_Picture_21.jpg" style="margin-top:12px; width:100%; border-radius:6px">
      </div>
      <div class="card fragment">
        <h3>Prompt Transformation</h3>
        <p>Increase and diversify the dataset to shape how prompts are interpreted</p>
        <img src="img/slide7_Content_Placeholder_10.jpg" style="margin-top:12px; width:100%; border-radius:6px">
      </div>
    </div>
  </div>

  <div class="slide" data-notes="A final video to reflect on. After everything we've discussed — the tools, the harms, the creativity, the exploitation — where do we go from here? Play the video and let it sink in before opening discussion.">
    <iframe src="https://www.youtube-nocookie.com/embed/ZU767J6p96o" style="width:80%; max-width:700px; aspect-ratio:16/9; border:none; border-radius:8px" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen></iframe>
  </div>

  <!-- ============================================ -->
  <!-- GENERATIVE AI APPLICATIONS                    -->
  <!-- ============================================ -->

  <div class="slide section-title" data-notes="Now let's look at some of the applications of generative AI — what these tools can actually do today.">
    <h2>Generative AI <span class="accent">Applications</span></h2>
    <div class="divider"></div>
  </div>

  <div class="slide" data-notes="Overview of what generative AI can produce today. The key modalities are text-to-image, text-to-video, image-to-image transformation, image-to-video animation, and combined image+text-to-video. Each has different tools and different levels of maturity.">
    <h2>Generative AI Applications</h2>
    <h3>Image &amp; Video Synthesis</h3>
    <div class="three-col" style="max-width:800px; margin-top:16px">
      <div class="card"><p style="color:var(--text)">Text-to-Image</p></div>
      <div class="card"><p style="color:var(--text)">Text-to-Video</p></div>
      <div class="card"><p style="color:var(--text)">Image-to-Image</p></div>
      <div class="card"><p style="color:var(--text)">Image-to-Video</p></div>
      <div class="card" style="grid-column: span 2"><p style="color:var(--text)">Image+Text-to-Video</p></div>
    </div>
  </div>

  <!-- ============================================ -->
  <!-- PART 4: RESOURCE APPROPRIATION                -->
  <!-- ============================================ -->

  <div class="slide section-title" data-notes="Like colonial powers claiming land and resources that weren't theirs, AI companies have laid claim to the world's data, creative work, and natural resources — scraping billions of people's work without permission or compensation to build proprietary systems. This is resource appropriation at industrial scale.">
    <h2>Resource <span class="red">Appropriation</span></h2>
    <div class="divider" style="background:var(--red)"></div>
  </div>

  <div class="slide" data-notes="To understand the scale of extraction: companies like OpenAI, Google, and Meta built web scrapers that hoovered up the entire internet — text, images, code, music, personal data — without asking anyone. Common Crawl alone contains petabytes of web data. Books3 contained over 190,000 pirated books. LAION scraped billions of images from Flickr, DeviantArt, and personal blogs. This isn't incidental data collection — it's industrial-scale extraction of human creative output.">
    <h2>Data Extraction</h2>
    <div class="two-col" style="max-width:900px; align-items:start">
      <div class="card red-border">
        <h3 style="color:var(--red)">What Was Taken</h3>
        <ul>
          <li><strong>Text</strong> — books, articles, blog posts, forum discussions, personal websites</li>
          <li><strong>Images</strong> — art, photography, medical scans, personal photos</li>
          <li><strong>Code</strong> — open-source repositories, private snippets</li>
          <li><strong>Audio</strong> — music, podcasts, voice recordings</li>
        </ul>
      </div>
      <div class="card yellow-border fragment">
        <h3 style="color:var(--yellow)">The Scale</h3>
        <ul>
          <li><strong>Common Crawl</strong> — petabytes of web data</li>
          <li><strong>Books3</strong> — 190,000+ pirated books</li>
          <li><strong>LAION-5B</strong> — 5 billion image-text pairs</li>
          <li><strong>The Pile</strong> — 800GB of diverse text</li>
        </ul>
      </div>
    </div>
    <p class="fragment" style="margin-top:20px; color:var(--red); font-size:1.2rem">No permission. No compensation. No opt-out.</p>
  </div>

  <div class="slide" data-notes="Text extraction at scale. Books3 was a dataset of 190,000+ pirated books used to train Meta's LLaMA and other models — including works by Stephen King, Margaret Atwood, and Zadie Smith. The New York Times sued OpenAI in December 2023 after discovering ChatGPT could reproduce Times articles nearly verbatim. Reddit sold its entire archive of user posts to Google for $60 million in 2024 — 18 years of human conversations monetized without the users who wrote them seeing a cent. Stack Overflow's community of developers spent years building a knowledge base that was scraped to train coding assistants like GitHub Copilot — which now competes with Stack Overflow itself. The Authors Guild found that the vast majority of published books in the US appear in AI training sets without permission.">
    <h2>Text</h2>
    <div class="two-col" style="max-width:950px; align-items:start">
      <div class="card red-border">
        <h3 style="color:var(--red)">What Was Taken</h3>
        <ul>
          <li><strong>Books3</strong> — 190,000+ pirated books (Stephen King, Atwood, Zadie Smith)</li>
          <li><strong>Reddit</strong> — entire archive sold to Google for $60M</li>
          <li><strong>Stack Overflow</strong> — scraped to train Copilot, which now competes with it</li>
          <li><strong>News articles</strong> — NYT sued OpenAI after verbatim reproduction</li>
        </ul>
      </div>
      <div class="card yellow-border fragment">
        <h3 style="color:var(--yellow)">Who Was Harmed</h3>
        <ul>
          <li>Authors whose books were pirated</li>
          <li>Journalists whose reporting was absorbed</li>
          <li>Reddit users — 18 years of posts monetized, $0 to writers</li>
          <li>Developers who built Stack Overflow's knowledge base for free</li>
        </ul>
      </div>
    </div>
  </div>

  <div class="slide" data-notes="Image extraction. LAION-5B scraped 5 billion image-text pairs from the open web — including Flickr, DeviantArt, ArtStation, personal blogs, and even medical databases. Artists like Greg Rutkowski, Kim Jung Gi, and Sam Yang found their names were among the most-used prompts in Stable Diffusion — their distinctive styles were extracted and made reproducible by anyone. Getty Images sued Stability AI in 2023 after finding Getty-watermarked images in training data. The Stanford Internet Observatory found that LAION contained child sexual abuse material (CSAM). Photographers discovered their copyrighted images appearing in AI outputs with slight modifications. Concept artists at studios reported losing work because clients could now generate 'in the style of' their portfolio for free.">
    <h2>Images</h2>
    <div class="two-col" style="max-width:950px; align-items:start">
      <div class="card red-border">
        <h3 style="color:var(--red)">What Was Taken</h3>
        <ul>
          <li><strong>LAION-5B</strong> — 5 billion images from Flickr, DeviantArt, ArtStation</li>
          <li><strong>Getty Images</strong> — watermarked photos found in training data</li>
          <li><strong>Named artists</strong> — Greg Rutkowski, Kim Jung Gi among top prompts</li>
          <li><strong>Medical images</strong> — patient scans scraped without consent</li>
        </ul>
      </div>
      <div class="card yellow-border fragment">
        <h3 style="color:var(--yellow)">Who Was Harmed</h3>
        <ul>
          <li>Artists whose styles became free prompts</li>
          <li>Concept artists losing work to "in the style of" generation</li>
          <li>Photographers whose copyrighted work was absorbed</li>
          <li>Children — CSAM found in LAION dataset</li>
        </ul>
      </div>
    </div>
  </div>

  <div class="slide" data-notes="Code extraction. GitHub Copilot was trained on all public GitHub repositories — including code with restrictive licenses like GPL that require derivative works to be open-sourced. Microsoft/GitHub/OpenAI faced a class action lawsuit from developers who argued their copyrighted code was used without permission. Copilot has been caught reproducing exact code snippets, including license headers and copyright notices — proving it memorized rather than 'learned from' the training data. The irony: open-source developers shared their code freely for the commons, and a trillion-dollar company turned it into a paid product. Some developers have since moved to licenses that explicitly prohibit AI training.">
    <h2>Code</h2>
    <div class="two-col" style="max-width:950px; align-items:start">
      <div class="card red-border">
        <h3 style="color:var(--red)">What Was Taken</h3>
        <ul>
          <li><strong>All public GitHub repos</strong> — including GPL-licensed code</li>
          <li>Copilot reproduces <strong>exact snippets</strong> with license headers</li>
          <li>Code shared for the <strong>open-source commons</strong> turned into a paid product</li>
        </ul>
      </div>
      <div class="card yellow-border fragment">
        <h3 style="color:var(--yellow)">Who Was Harmed</h3>
        <ul>
          <li>Open-source developers — code shared freely, monetized by Microsoft</li>
          <li>GPL authors — derivative works should be open, Copilot is not</li>
          <li>Class action lawsuit filed against GitHub, Microsoft, OpenAI</li>
        </ul>
      </div>
    </div>
  </div>

  <div class="slide" data-notes="Audio extraction. OpenAI built Whisper by transcribing over 1 million hours of YouTube videos — employees internally acknowledged this likely violated YouTube's terms of service. Spotify has seen a flood of AI-generated music uploaded by bots to harvest streaming royalties, diluting payments to real musicians. Voice cloning services like ElevenLabs only need a few seconds of audio to clone a voice — and voices scraped from podcasts, audiobooks, and social media are being used without consent. Voice actors have found AI versions of their voices being sold commercially without their knowledge. The music industry is seeing AI-generated songs 'in the style of' specific artists — Drake and The Weeknd deepfake tracks went viral before being removed. Audiobook narrators are being replaced by AI voices trained on their own recordings.">
    <h2>Audio</h2>
    <div class="two-col" style="max-width:950px; align-items:start">
      <div class="card red-border">
        <h3 style="color:var(--red)">What Was Taken</h3>
        <ul>
          <li><strong>1M+ hours</strong> of YouTube transcribed by OpenAI's Whisper</li>
          <li><strong>Podcasts &amp; audiobooks</strong> — voices cloned from seconds of audio</li>
          <li><strong>Music</strong> — AI-generated Drake/Weeknd tracks went viral</li>
          <li><strong>Voice actors</strong> — AI versions sold without their knowledge</li>
        </ul>
      </div>
      <div class="card yellow-border fragment">
        <h3 style="color:var(--yellow)">Who Was Harmed</h3>
        <ul>
          <li>Musicians — AI bots flood Spotify, diluting royalties</li>
          <li>Voice actors — replaced by AI trained on their own recordings</li>
          <li>Audiobook narrators — losing work to cloned voices</li>
          <li>YouTubers — content transcribed without consent for GPT-4</li>
        </ul>
      </div>
    </div>
  </div>

  <div class="slide" data-notes="This is from a New York Times investigation published April 2024. In January 2020, Jared Kaplan published a paper showing that more data equals better AI — 'scale is all you need' became the rallying cry. By late 2021, OpenAI had exhausted every reputable English-language text source on the internet. They built Whisper to transcribe over 1 million hours of YouTube videos — even though employees knew it likely violated YouTube's rules. Google was doing the same thing with YouTube transcripts. Meta's VP of generative AI told executives they'd used almost every available English book, essay, poem and news article. Meta discussed buying Simon & Schuster outright just for the training data. One Meta lawyer warned of ethical concerns around taking artists' IP and was met with silence. Google quietly broadened its privacy policy on July 4th weekend 2023 to allow using Google Docs and Maps data for AI. Researchers estimate companies could exhaust all high-quality internet data by 2026.">
    <h2>How Tech Giants Harvested Data</h2>
    <div style="display:flex; gap:20px; align-items:start; max-width:1100px; width:100%">
      <img src="img/nyt_harvest_data.png" style="max-width:500px; width:100%; border-radius:8px; object-fit:contain; flex-shrink:0">
      <div style="text-align:left">
        <div class="card" style="padding:14px 18px; margin-bottom:8px">
          <p style="color:var(--dim); font-size:1.25rem"><strong class="yellow">Jan 2020</strong> — "Scale is all you need" paper published. More data = better AI becomes the rallying cry</p>
        </div>
        <div class="card fragment" style="padding:14px 18px; margin-bottom:8px; border-left-color:var(--red)">
          <p style="color:var(--dim); font-size:1.25rem"><strong class="red">Late 2021</strong> — OpenAI exhausts every reputable English-language text on the internet</p>
        </div>
        <div class="card fragment" style="padding:14px 18px; margin-bottom:8px; border-left-color:var(--red)">
          <p style="color:var(--dim); font-size:1.25rem"><strong class="red">2022</strong> — OpenAI builds Whisper, transcribes <strong>1M+ hours</strong> of YouTube videos for GPT-4. Employees knew it likely violated YouTube's rules</p>
        </div>
        <div class="card fragment" style="padding:14px 18px; margin-bottom:8px; border-left-color:var(--orange)">
          <p style="color:var(--dim); font-size:1.25rem"><strong class="orange">2023</strong> — Meta uses almost every English book, essay &amp; article online. Discusses buying Simon &amp; Schuster for training data</p>
        </div>
        <div class="card fragment" style="padding:14px 18px; margin-bottom:8px; border-left-color:var(--yellow)">
          <p style="color:var(--dim); font-size:1.25rem"><strong class="yellow">July 4 2023</strong> — Google quietly broadens privacy policy to use Google Docs, Maps &amp; other user data for AI</p>
        </div>
        <div class="card fragment" style="padding:14px 18px; border-left-color:var(--dim)">
          <p style="color:var(--dim); font-size:1.25rem"><strong style="color:var(--text)">~2026</strong> — Estimated date companies exhaust all high-quality internet data</p>
        </div>
      </div>
    </div>
  </div>

  <div class="slide" data-notes="Two emerging responses. Have I Been Trained lets artists check whether their work appears in AI training sets — a kind of data audit tool. Invisible watermarking embeds imperceptible markers in images so provenance can be tracked even after the image is used in training. Neither is a complete solution, but both represent important steps.">
    <h2>Copyright Violations in Datasets</h2>
    <div class="two-col" style="max-width:800px">
      <div class="card">
        <h3>Have I Been Trained?</h3>
        <p><a href="https://haveibeentrained.com/" target="_blank">haveibeentrained.com</a></p>
        <p style="margin-top:8px">Check if your work was used to train AI models</p>
      </div>
      <div class="card yellow-border fragment">
        <h3 style="color:var(--yellow)">Invisible Watermarking</h3>
        <p>Embedding invisible markers to track provenance of creative works</p>
      </div>
    </div>
  </div>

  <div class="slide" data-notes="OpenAI's Copyright Shield is a corporate response — they promise to cover legal costs if users are sued for copyright infringement from AI-generated content. Interesting move: it shifts liability from the user to the company, but doesn't address the original creators whose work was used in training.">
    <h2>OpenAI Copyright Shield</h2>
    <div style="text-align:center">
      <img src="img/slide12_Picture_5.png" class="medium">
    </div>
  </div>

  <div class="slide" data-notes="LAION-5B is one of the largest open datasets used to train image generation models like Stable Diffusion. 5 billion image-text pairs scraped from the web. It was later found to contain child sexual abuse material — a devastating discovery that led to the dataset being temporarily taken down. This highlights how scale without oversight creates real harm.">
    <h2>LAION-5B Dataset</h2>
    <div class="card red-border" style="max-width:700px">
      <ul>
        <li><strong>5 billion</strong> image-text pairs</li>
        <li>Sourced from Pinterest, WordPress, Blogspot, Flickr, DeviantArt &amp; Wikimedia Commons</li>
        <li class="fragment" style="opacity:0"><strong class="red">Found to contain child sexual abuse material (CSAM)</strong></li>
      </ul>
    </div>
  </div>

  <!-- ============================================ -->
  <!-- PART 5: SCALE AT ALL COSTS                    -->
  <!-- ============================================ -->

  <div class="slide section-title" data-notes="Imperial expansion always demands more — more land, more water, more energy, more resources. AI expansion follows the same logic: data centres are built where land and energy are cheap, not where the people who profit from AI live. The environmental burden is externalized onto communities with the least power, just as colonial industries dumped waste and extracted resources from colonized lands.">
    <h2>Scale at <span class="green">All Costs</span></h2>
    <div class="divider" style="background:var(--green)"></div>
  </div>

  <div class="slide" data-notes="This is xAI's Colossus data centre in Memphis, Tennessee — the largest AI supercomputer in the world. It was built in just 122 days, running on gas turbines that burn natural gas 24/7. Residents in the surrounding neighbourhood — a predominantly Black community — reported vibrations, constant noise, and air quality concerns. The facility consumes as much energy as a small city. This is what 'scale at all costs' looks like on the ground: imperial infrastructure built in marginalized communities to power AI systems those communities will never benefit from. Photo credit: Steve Jones / Flight by Southwings for SELC.">
    <h2>Data Centers</h2>
    <div class="two-col" style="max-width:1000px; align-items:start">
      <img src="img/colossus_datacenter.avif" style="width:100%; border-radius:8px; object-fit:contain">
      <img src="img/xai_datacenter_aerial.jpg" style="width:100%; border-radius:8px; object-fit:contain">
    </div>
    <p style="color:var(--dim); font-size:1rem; margin-top:12px">xAI Colossus — Memphis, Tennessee. The world's largest AI supercomputer, built in 122 days.</p>
  </div>

  <div class="slide" data-notes="Data centres aren't new — they've been powering the internet for decades. Everything from cloud services, streaming, banking, gaming, and e-commerce runs on data centres. The point: AI is not the first heavy user of this infrastructure, but it is changing the game significantly.">
    <h2>Data Centers</h2>
    <div class="tool-grid" style="max-width:900px">
      <div class="card"><h3>Cloud Computing &amp; SaaS</h3><p>AWS, Azure, Google Cloud, Teams, Office 365, Slack</p></div>
      <div class="card green-border"><h3 style="color:var(--green)">Web &amp; Content Delivery</h3><p>Netflix, YouTube, Spotify, social media platforms</p></div>
      <div class="card yellow-border"><h3 style="color:var(--yellow)">Enterprise IT</h3><p>Corporate email, file storage, internal applications</p></div>
      <div class="card pink-border"><h3 style="color:var(--pink)">Banking &amp; Finance</h3><p>High-frequency trading, payment processing (Visa, Mastercard)</p></div>
      <div class="card orange-border"><h3 style="color:var(--orange)">Gaming</h3><p>Multiplayer servers, game streaming (Xbox Cloud, GeForce Now)</p></div>
      <div class="card red-border"><h3 style="color:var(--red)">E-Commerce &amp; Crypto</h3><p>Amazon, Shopify, Bitcoin mining, blockchain</p></div>
    </div>
  </div>

  <div class="slide" data-notes="Here's the key comparison. Traditional workloads use CPUs, air cooling, and have variable demand. AI workloads use dense GPU clusters that run flat-out 24/7, generating so much heat they need liquid water cooling. The growth rate is the headline: traditional data centre demand grew steadily over decades, but AI demand is growing exponentially — some projections say it will double total data centre energy use within 5 years.">
    <h2>AI &amp; Data Centers</h2>
    <table class="data-table" style="max-width:900px">
      <thead>
        <tr><th></th><th>Traditional Applications</th><th>AI Applications</th></tr>
      </thead>
      <tbody>
        <tr><td>Power</td><td>Only require CPU</td><td>Requires high-end GPU/TPU clusters</td></tr>
        <tr><td>Heat &amp; Cooling</td><td>Traditional air cooling</td><td>Liquid water cooling (higher density packing)</td></tr>
        <tr><td>Usage Patterns</td><td>Ebbs &amp; Flows</td><td>Runs 24/7, no idle time</td></tr>
        <tr><td>Growth Rate</td><td>Grown steadily over decades</td><td>Growing exponentially</td></tr>
      </tbody>
    </table>
  </div>

  <div class="slide" data-notes="Video showing the scale and impact of AI data centre infrastructure. Let the video speak for itself — then discuss with students.">
    <video src="img/datacentre_video.mp4" controls style="max-height:70vh; max-width:80%; border-radius:8px"></video>
  </div>

  <!-- ============================================ -->
  <!-- PART 6: LABOR EXPLOITATION                    -->
  <!-- ============================================ -->

  <div class="slide section-title" data-notes="Empires have always relied on cheap or coerced labor — from indentured workers in British colonies to enslaved people in the Americas. Today's AI empires follow the same pattern: content moderators in Kenya earn $2/hour cleaning up traumatic training data, while the companies they serve are valued in the trillions. The wealth flows upward; the human cost is borne by those with the least power.">
    <h2>Labor <span class="orange">Exploitation</span></h2>
    <div class="divider" style="background:var(--orange)"></div>
  </div>

  <div class="slide" data-notes="AI's invisible workforce. Sama (formerly Samasource) employed workers in Nairobi, Kenya to review horrific content — beheadings, child abuse, bestiality — for $2/hour so ChatGPT could have safety filters. TIME's 2023 investigation revealed workers developed PTSD, depression, and were offered minimal mental health support. When they organized for better conditions, contracts were terminated. Meanwhile, OpenAI's valuation went from $29 billion to $150 billion. Data annotators — often in India, the Philippines, Venezuela, and refugee camps — label millions of images, correct model outputs, and verify AI responses for pennies per task. Scale AI, Appen, and Remotasks pay as little as $1-3/hour. A University of Oxford study found that AI data work recreates colonial labor patterns: the Global South does the tedious, traumatic work; the Global North captures the value. These workers are deliberately kept invisible — they're classified as independent contractors with no benefits, no job security, and no credit.">
    <h2>The Invisible <span class="orange">Workforce</span></h2>
    <div class="three-col" style="max-width:1000px; gap:16px">
      <div class="card orange-border">
        <h3 style="color:var(--orange)">Content Moderators</h3>
        <ul>
          <li>$2/hour in Nairobi, Kenya</li>
          <li>Review beheadings, CSAM, self-harm</li>
          <li>PTSD and depression widespread</li>
          <li>Contracts terminated when workers organized</li>
        </ul>
        <p style="color:var(--dim); font-size:0.8rem; margin-top:8px">Source: TIME investigation, 2023</p>
      </div>
      <div class="card red-border fragment">
        <h3 style="color:var(--red)">Data Annotators</h3>
        <ul>
          <li>$1–3/hour in India, Philippines, Venezuela</li>
          <li>Label millions of images & text</li>
          <li>No benefits, no job security</li>
          <li>Classified as "independent contractors"</li>
        </ul>
        <p style="color:var(--dim); font-size:0.8rem; margin-top:8px">Scale AI, Appen, Remotasks</p>
      </div>
      <div class="card yellow-border fragment">
        <h3 style="color:var(--yellow)">Mineral Miners</h3>
        <ul>
          <li>Cobalt mines in DRC — 40,000 children</li>
          <li>Lithium extraction in Chile, Argentina</li>
          <li>Rare earths in Myanmar, China</li>
          <li>Powers the GPUs that run AI</li>
        </ul>
        <p style="color:var(--dim); font-size:0.8rem; margin-top:8px">Source: Amnesty International</p>
      </div>
    </div>
  </div>

  <div class="slide" data-notes="Video showing workers being displaced by AI. Concept artists, illustrators, copywriters, translators, voice actors, customer service agents — entire professions being devalued or eliminated in months, not decades. Let the video play, then discuss: who benefits from this displacement, and who bears the cost?">
    <h2>Displaced by <span class="red">AI</span></h2>
    <video src="img/ai_displacement_video.mp4" controls style="max-height:60vh; max-width:80%; border-radius:8px; margin-top:12px"></video>
  </div>

  <div class="slide" data-notes="The pattern is identical to colonial labor structures. In the Belgian Congo, rubber was extracted by coerced local labor while profits flowed to Brussels. Today, cobalt is mined by hand in the DRC — including by an estimated 40,000 children according to UNICEF — to make the batteries and chips that power AI infrastructure. The key numbers: Sama's content moderators earned roughly $2/hour while OpenAI's valuation grew from $29B to $150B. Data annotators on Remotasks earn $1-3/hour while Scale AI is valued at $14 billion. A cobalt miner in Kolwezi earns $2-3/day while Nvidia's market cap hit $3 trillion. The wealth ratio is staggering — the people doing the most dangerous and psychologically damaging work capture the smallest fraction of value. Oxford researchers call this 'digital colonialism': the same extraction patterns, the same geography, the same power dynamics.">
    <h2>The <span class="red">Colonial</span> Pattern</h2>
    <p style="color:var(--dim); margin-bottom:20px; font-size:1.1rem">The wealth flows up. The harm flows down.</p>
    <div class="two-col" style="max-width:900px; align-items:start">
      <div class="card" style="border-left-color:var(--orange)">
        <h3>The Workers</h3>
        <ul>
          <li>Content moderator in Kenya: <strong style="color:var(--orange)">$2/hour</strong></li>
          <li>Data annotator in India: <strong style="color:var(--orange)">$1–3/hour</strong></li>
          <li>Cobalt miner in DRC: <strong style="color:var(--orange)">$2–3/day</strong></li>
        </ul>
      </div>
      <div class="card fragment" style="border-left-color:var(--green)">
        <h3>The Companies</h3>
        <ul>
          <li>OpenAI valuation: <strong style="color:var(--green)">$150 billion</strong></li>
          <li>Scale AI valuation: <strong style="color:var(--green)">$14 billion</strong></li>
          <li>Nvidia market cap: <strong style="color:var(--green)">$3 trillion</strong></li>
        </ul>
      </div>
    </div>
  </div>

  <!-- ============================================ -->
  <!-- THE CIVILIZING MISSION                        -->
  <!-- ============================================ -->

  <div class="slide section-title" data-notes="Colonial empires justified their expansion through a 'civilizing mission' — claiming to bring progress, Christianity, and modernity to 'backward' peoples. Today's AI empires use the same logic: 'democratizing AI', 'benefiting all of humanity', 'making knowledge accessible'. But whose knowledge? Whose culture? Models trained on Western, English-language data flatten the world's diversity into a single homogenized output — a digital civilizing mission at planetary scale.">
    <h2>The Civilizing <span class="pink">Mission</span></h2>
    <div class="divider" style="background:var(--pink)"></div>
  </div>

  <div class="slide" data-notes="Washington Post (2024): when prompted with 'beautiful woman', nearly 9 in 10 of Midjourney's outputs are light-skinned. Only 2% of images showed single-fold eyelids (common in East Asian faces). University of Washington study: prompting 'a person' over-represents light-skinned men and sexualizes women of colour. Bloomberg (2023): Stable Diffusion defaults to white male CEOs, white doctors, white engineers. The world according to these models is Western, white, and male unless you explicitly specify otherwise. This is what a civilizing mission looks like in the 21st century — not missionaries and schools, but training data and default outputs that encode one culture's aesthetics as universal.">
    <h2>The Default is Western</h2>
    <div class="two-col" style="max-width:950px; align-items:start">
      <div>
        <div class="card pink-border" style="margin-bottom:12px">
          <h3 style="color:var(--pink)">Prompt: "beautiful woman"</h3>
          <p><strong class="pink">9 in 10</strong> outputs are light-skinned (Midjourney)</p>
          <p>Only <strong class="pink">2%</strong> showed single-fold eyelids</p>
          <p style="color:var(--dim); font-size:0.9rem; margin-top:8px">— Washington Post, 2024</p>
        </div>
        <div class="card red-border fragment" style="margin-bottom:12px">
          <h3 style="color:var(--red)">Prompt: "a person"</h3>
          <p>Defaults to <strong>light-skinned men</strong></p>
          <p>Women of colour are <strong>sexualized</strong></p>
          <p style="color:var(--dim); font-size:0.9rem; margin-top:8px">— University of Washington, 2023</p>
        </div>
        <div class="card yellow-border fragment">
          <h3 style="color:var(--yellow)">Prompt: "a CEO"</h3>
          <p>Stable Diffusion: almost exclusively <strong>white men</strong></p>
          <p style="color:var(--dim); font-size:0.9rem; margin-top:8px">— Bloomberg, 2023</p>
        </div>
      </div>
      <div>
        <img src="img/midjourney_general_grid.jpg" style="width:100%; border-radius:8px; margin-bottom:12px">
        <p style="color:var(--dim); font-size:0.9rem; text-align:center">Midjourney outputs for neutral prompts across cultures<br><em>Source: Rest of World, 2023</em></p>
      </div>
    </div>
  </div>

  <div class="slide" data-notes="These are actual Midjourney outputs from a Rest of World investigation (2023). They prompted 'an Indian person', 'a Mexican person', 'a Nigerian person' — the results reduce entire cultures to the most stereotypical Western imagination of them. Every Indian person wears traditional clothing, every Mexican is in a sombrero or colorful shawl, every Nigerian is in bright traditional dress. Meanwhile 'an American person' produces casual, modern, diverse-looking white people. The models don't reflect reality — they reflect the internet's stereotypes, which are overwhelmingly produced by Western, English-speaking users. This is cultural flattening at scale. And then when Google tried to fix it with Gemini — adding a diversity filter that generated Black Nazi soldiers and Asian US founding fathers — the overcorrection revealed how superficial the fix was. You can't patch over structural bias with a diversity checkbox.">
    <h2>AI Stereotypes by Country</h2>
    <div class="two-col" style="max-width:90%; align-items:start; gap:24px">
      <div>
        <img src="img/midjourney_indian_person.jpg" style="width:100%; border-radius:8px; margin-bottom:8px">
        <p style="color:var(--dim); font-size:0.9rem; text-align:center">Midjourney: "an Indian person"</p>
      </div>
      <div>
        <img src="img/midjourney_mexican_person.jpg" style="width:100%; border-radius:8px; margin-bottom:8px">
        <p style="color:var(--dim); font-size:0.9rem; text-align:center">Midjourney: "a Mexican person"</p>
      </div>
    </div>
    <div class="card fragment" style="max-width:800px; margin-top:16px; border-left-color:var(--pink)">
      <p><strong class="pink">Google Gemini's "fix":</strong> a diversity filter that generated Black Nazi soldiers and Asian US founding fathers — paused within days after backlash. You can't patch structural bias with a checkbox.</p>
    </div>
  </div>

  <!-- ============================================ -->
  <!-- COMPETITIVE JUSTIFICATION                     -->
  <!-- ============================================ -->

  <div class="slide section-title" data-notes="Every empire justifies its expansion by pointing to a rival empire. The British said 'if we don't colonize Africa, France will.' Today's AI companies say 'we must build AGI before China does.' This competitive justification drives a race to deploy powerful AI systems with minimal safeguards — because slowing down means losing. The result: a handful of corporations concentrate unprecedented power while framing it as national security necessity.">
    <h2>Competitive <span class="accent">Justification</span></h2>
    <div class="divider"></div>
  </div>

  <div class="slide" data-notes="The AI arms race mirrors colonial scrambles for territory. Sam Altman has explicitly framed AI development as a geopolitical competition — 'American AI vs Chinese AI.' This framing serves a strategic purpose: it makes safety concerns sound like weakness, turns reckless speed into patriotic duty, and justifies concentrating enormous power in a few private companies. Microsoft invested $13 billion in OpenAI. Google, Meta, and Amazon are each spending over $50 billion per year on AI infrastructure. These companies argue they need minimal regulation because slowing down means 'losing to China.' But the real competition isn't between nations — it's between corporations racing to lock in market dominance. The same logic drove the East India Company to expand faster than its rivals, the same logic drove the scramble for Africa. The justification is always the same: 'if we don't do it, someone worse will.'">
    <h2>The AI Arms Race</h2>
    <div class="two-col" style="max-width:900px">
      <div class="card" style="border-left-color:var(--accent)">
        <h3 style="color:var(--accent)">The Logic</h3>
        <ul>
          <li>"We must build AGI before China does"</li>
          <li>Safety concerns reframed as <strong>weakness</strong></li>
          <li>Reckless speed becomes <strong>patriotic duty</strong></li>
          <li>Minimal regulation = competitive advantage</li>
        </ul>
      </div>
      <div class="card red-border fragment">
        <h3 style="color:var(--red)">The Reality</h3>
        <ul>
          <li>Microsoft: <strong>$13B</strong> into OpenAI</li>
          <li>Google, Meta, Amazon: <strong>$50B+/year</strong> each on AI infrastructure</li>
          <li>A handful of companies concentrate <strong>unprecedented power</strong></li>
          <li>The race isn't between nations — it's between <strong>corporations</strong></li>
        </ul>
      </div>
    </div>
  </div>

  <div class="slide" data-notes="This is the national framing in action. The US, China, and the Netherlands form a triangle that reveals how AI competition plays out geopolitically. The US framing: Trump signed an executive order to 'maintain American AI dominance'. The CHIPS Act allocated $52 billion — framed as national security. Sam Altman testified to Congress that the US must lead or 'authoritarian regimes' will. China's response: State Council declared AI a strategic priority in 2017. DeepSeek released open-source models rivalling GPT-4. After US chip bans, China is pushing hard for semiconductor self-sufficiency. The Netherlands is the hidden kingmaker: ASML in Veldhoven is the only company on Earth that makes EUV lithography machines — the equipment needed to manufacture the most advanced AI chips. Without ASML, neither Nvidia, TSMC, nor Samsung can make cutting-edge processors. The US pressured the Dutch government to restrict ASML exports to China — the Netherlands effectively became a weapon in the AI arms race. ASML's machines cost $380 million each and take 40 shipping containers to deliver. This is a chokepoint: one company in Brabant controls the bottleneck of the entire global AI supply chain. The pattern: governments frame it as national security, but the money flows to private corporations. Nvidia's market cap tripled. OpenAI went from nonprofit to $150B valuation. ASML's stock surged. The 'China threat' narrative justifies unchecked expansion and silences critics.">
    <h2>The Geopolitical Triangle</h2>
    <div class="three-col" style="max-width:1050px; align-items:start; gap:12px">
      <div class="card" style="border-left-color:var(--accent); padding:16px 18px">
        <h3 style="color:var(--accent); font-size:1.15rem">United States</h3>
        <ul>
          <li>"Maintain American AI dominance"</li>
          <li><strong>CHIPS Act</strong> — $52B</li>
          <li>Export controls on chips to China</li>
          <li>Altman to Congress: "lead or <strong>authoritarian regimes</strong> will"</li>
        </ul>
      </div>
      <div class="card red-border fragment" style="padding:16px 18px">
        <h3 style="color:var(--red); font-size:1.15rem">China</h3>
        <ul>
          <li>AI strategy since <strong>2017</strong></li>
          <li>Baidu, Alibaba, ByteDance</li>
          <li><strong>DeepSeek</strong> — rivalling GPT-4</li>
          <li>Self-sufficiency push after chip bans</li>
        </ul>
      </div>
      <div class="card orange-border fragment" style="padding:16px 18px">
        <h3 style="color:var(--orange); font-size:1.15rem">Netherlands</h3>
        <ul>
          <li><strong>ASML</strong> — only maker of EUV lithography machines</li>
          <li>Without ASML, no advanced AI chips</li>
          <li>US pressured Dutch export restrictions</li>
          <li>One company in Brabant controls the <strong>global bottleneck</strong></li>
        </ul>
      </div>
    </div>
    <div class="card fragment" style="max-width:900px; margin-top:14px; border-left-color:var(--yellow)">
      <p><strong class="yellow">The pattern:</strong> governments frame it as national security — but the money flows to <strong>private corporations</strong>. Nvidia's market cap tripled. OpenAI: nonprofit → $150B. ASML machines cost <strong>$380M each</strong>.</p>
    </div>
  </div>

  <div class="slide" data-notes="Google integrated their Veo3 video generation model directly into YouTube. This means AI-generated video is now a first-class feature on the world's largest video platform. The line between 'real' and 'generated' YouTube content becomes increasingly blurred.">
    <h2>Google + YouTube + Veo3</h2>
    <div style="display:flex; gap:24px; align-items:center; max-width:950px; width:100%">
      <iframe src="https://www.youtube-nocookie.com/embed/zrphf1DH6Nw" style="width:320px; aspect-ratio:9/16; border:none; border-radius:8px; flex-shrink:0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen></iframe>
      <div class="card" style="flex:1">
        <p style="color:var(--text)">Google's video generation model integrated into YouTube, enabling AI-generated video content at scale.</p>
      </div>
    </div>
  </div>

  <div class="slide" data-notes="Meta launched Vibes — generative AI tools built into Instagram and Facebook. Users can now create AI-generated images and videos directly within social media apps used by billions. This normalises synthetic content in everyday social interaction.">
    <h2>Meta + Vibes</h2>
    <img src="img/Vibes_header.gif" style="max-width:900px; width:100%; border-radius:8px; margin-bottom:16px; object-fit:contain">
    <div class="card" style="max-width:700px">
      <p style="color:var(--text)">Meta's generative AI tools integrated across Instagram and Facebook for content creation.</p>
    </div>
  </div>

  <div class="slide" data-notes="OpenAI launched Sora2 — their next-generation video model with higher fidelity, longer clips, and better prompt following. Unlike the original Sora which was limited access, Sora2 is being rolled out more broadly. The quality is reaching a point where casual viewers cannot distinguish it from real footage.">
    <h2>OpenAI + Sora2</h2>
    <div class="card" style="max-width:700px; margin-bottom:16px">
      <p style="color:var(--text)">OpenAI's next-generation video model — higher fidelity, longer clips, better prompt adherence.</p>
    </div>
  </div>

  <div class="slide" data-notes="Here are Sora2 examples in practice. Notice the quality — the lighting, physics, and human movement are remarkably realistic. We're rapidly approaching a world where seeing is no longer believing.">
    <h2>OpenAI + Sora2</h2>
    <div class="two-col" style="max-width:950px; align-items:center">
      <img src="img/slide74_Picture_3.png" style="width:100%; border-radius:8px; object-fit:contain">
      <video src="img/slide74_ScreenRecording_10-17-2025_16-10-28_1.mp4" controls style="width:100%; border-radius:8px"></video>
    </div>
  </div>

  <div class="slide" data-notes="More examples of what mainstream AI video generation can produce. These were all generated by consumer-accessible tools. The range of content — from humorous to disturbing — shows both the creative potential and the risk.">
    <h2>AI Video Generation Examples</h2>
    <div class="two-col" style="max-width:950px">
      <video src="img/slide75_GayHitler.mp4" controls style="width:100%; border-radius:8px"></video>
      <video src="img/slide75_ScreenRecording_10-12-2025_15-19-47_1.mp4" controls style="width:100%; border-radius:8px"></video>
    </div>
  </div>

  <!-- ============================================ -->
  <!-- PART 13: CONCLUSION                          -->
  <!-- ============================================ -->

  <div class="slide section-title" data-notes="Let's bring it all together. What have we learned, and what does it mean going forward?">
    <h2><span class="accent">Conclusion</span></h2>
    <div class="divider"></div>
  </div>

  <div class="slide" data-notes="The argument isn't that AI is inherently evil, but that the conditions of its production, deployment, and ownership replicate imperial power structures — appropriation of resources, exploitation of labor, competitive expansion at all costs, a civilizing mission that flattens diversity, and justification through rivalry. These are the patterns of empire, not coincidence.">
    <h2>The Empire of AI</h2>
    <div class="three-col" style="max-width:1000px; gap:12px">
      <div class="card red-border">
        <h3 style="color:var(--red)">Resource Appropriation</h3>
        <p>Training data scraped from billions without permission — laying claim to the world's creative output to build proprietary empires</p>
      </div>
      <div class="card orange-border fragment">
        <h3 style="color:var(--orange)">Labor Exploitation</h3>
        <p>$2/hour content moderators clean up the mess while trillion-dollar companies reap the profits — colonial labor patterns persist</p>
      </div>
      <div class="card yellow-border fragment">
        <h3 style="color:var(--yellow)">Competitive Justification</h3>
        <p>"We must build AGI before China does" — the same logic empires used to justify expansion by pointing to rival empires</p>
      </div>
      <div class="card pink-border fragment">
        <h3 style="color:var(--pink)">The Civilizing Mission</h3>
        <p>"Democratizing AI" and "benefiting humanity" — while encoding Western biases and flattening cultural diversity at planetary scale</p>
      </div>
      <div class="card green-border fragment">
        <h3 style="color:var(--green)">Scale at All Costs</h3>
        <p>Imperial expansion demanding more land, water, and energy — environmental burden externalized onto marginalized communities</p>
      </div>
    </div>
  </div>

  <!-- ============================================ -->
  <!-- BRIDGE: FROM EMPIRE TO IMPACT                 -->
  <!-- ============================================ -->

  <div class="slide" data-notes="The empire framework explains why deepfakes exist and proliferate. The deepfakes section shows who gets hurt — primarily women, primarily through non-consensual sexual content. It's the human cost of empire made visceral and personal. Resource Appropriation created the conditions — faces, voices, and likenesses were scraped without consent, making deepfakes possible in the first place. Competitive Justification means companies ship powerful generative video tools like Sora, Veo3, and Vibes with minimal safeguards — because slowing down means losing the race. The Civilizing Mission rhetoric of 'democratizing creativity' is exactly how these tools get marketed, even as 96% of deepfakes are non-consensual pornography targeting women. Scale at All Costs means the tools get better and cheaper faster than any regulation can keep up.">
    <h2>From Empire to <span class="red">Impact</span></h2>
    <p style="color:var(--dim); margin-bottom:24px; font-size:1.3rem">The empire builds the tools. Deepfakes show who gets hurt.</p>
    <div class="two-col" style="max-width:950px; align-items:start">
      <div class="card" style="border-left-color:var(--accent)">
        <h3 style="color:var(--accent)">The Empire Enables</h3>
        <ul>
          <li><strong class="red">Resource Appropriation</strong> — faces and voices scraped without consent</li>
          <li><strong class="yellow">Competitive Justification</strong> — tools shipped fast, safeguards slow</li>
          <li><strong class="pink">Civilizing Mission</strong> — "democratizing creativity"</li>
          <li><strong class="green">Scale at All Costs</strong> — tools get cheaper faster than regulation</li>
        </ul>
      </div>
      <div class="card red-border fragment">
        <h3 style="color:var(--red)">The Human Cost</h3>
        <ul>
          <li><strong>96%</strong> of deepfakes are non-consensual pornography</li>
          <li>Targets are overwhelmingly <strong>women</strong></li>
          <li>Victims have almost <strong>no legal recourse</strong></li>
          <li>The tools are free, the damage is <strong>permanent</strong></li>
        </ul>
      </div>
    </div>
  </div>

  <!-- ============================================ -->
  <!-- PART 8: WHAT ARE DEEPFAKES                   -->
  <!-- ============================================ -->

  <div class="slide section-title" data-notes="Now we shift gears. We've seen the tools — now let's talk about deepfakes specifically. What are they, where did they come from, and why should we care?">
    <h2>What Are <span class="red">Deepfakes</span></h2>
    <div class="divider" style="background:var(--red)"></div>
  </div>

  <div class="slide" data-notes="Let's start with a deepfake video example. Watch carefully — can you tell this isn't real? The term 'deepfake' combines 'deep learning' with 'fake'. It was coined on Reddit in 2017.">
    <h2>Deepfakes</h2>
    <video src="img/slide38_WhatsApp_Video_2025-03-29_at_13.50.56.mp4" controls style="width:80%; max-width:700px; border-radius:8px"></video>
  </div>

  <!-- ============================================ -->
  <!-- DEEPFAKE TYPES & ORIGINS                      -->
  <!-- ============================================ -->

  <div class="slide" data-notes="The origins of deepfakes: the term was coined by a Reddit user called 'deepfakes' in 2017 who posted face-swapped celebrity pornography. From the very beginning, this technology was intertwined with non-consensual sexual content. The subreddit was eventually banned, but the technology had already spread.">
    <h2>Origins</h2>
    <div class="img-row">
      <img src="img/slide42_Picture_2.jpg" class="medium">
      <img src="img/slide42_Picture_4.jpg" class="medium">
    </div>
  </div>

  <div class="slide" data-notes="Faceswaps are the most recognisable form of deepfake. A source face is mapped onto a target video. Modern faceswaps can handle different angles, lighting, and expressions in real time. Play the video to demonstrate.">
    <h2>Faceswaps</h2>
    <video src="img/slide43_WhatsApp_Video_2025-03-29_at_13.50.56.mp4" controls style="width:80%; max-width:700px; border-radius:8px"></video>
  </div>

  <div class="slide" data-notes="Voiceswaps clone someone's voice from audio samples and then generate new speech in that voice. This has been used in scams — for example, cloning a CEO's voice to authorize a fraudulent bank transfer. The technology only needs a few seconds of sample audio to produce a convincing clone.">
    <h2>Voiceswaps</h2>
    <iframe src="https://www.youtube-nocookie.com/embed/5cbCYwgQkTE?start=335" style="width:80%; max-width:700px; aspect-ratio:16/9; border:none; border-radius:8px" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen></iframe>
  </div>

  <div class="slide" data-notes="Textswaps are less discussed but equally important. AI can generate text that mimics a specific person's writing style, or alter existing text in images and documents. This enables forged messages, fake social media posts, and document manipulation.">
    <h2>Textswaps</h2>
    <img src="img/slide45_Picture_8.png" class="medium">
  </div>

  <div class="slide" data-notes="These are photos from my DEEPFAKE exhibition at Peckham Digital — an art project exploring the cultural implications of deepfake technology. Art can be a powerful way to make these issues tangible and accessible.">
    <h2>DEEPFAKE @ Peckham Digital</h2>
    <iframe src="https://player.vimeo.com/video/1082448682?badge=0&autopause=0&player_id=0&app_id=58479" style="width:80%; max-width:750px; aspect-ratio:16/9; border:none; border-radius:8px" allow="autoplay; fullscreen; picture-in-picture; clipboard-write; encrypted-media" allowfullscreen></iframe>
    <div style="display:flex; gap:16px; justify-content:center; margin-top:16px">
      <img src="img/slide39_Picture_6.jpg" style="max-height:200px; border-radius:8px; object-fit:contain">
      <img src="img/slide39_Picture_8.jpg" style="max-height:200px; border-radius:8px; object-fit:contain">
    </div>
  </div>

  <!-- ============================================ -->
  <!-- PART 10: DEEPFAKE APPLICATIONS               -->
  <!-- ============================================ -->

  <div class="slide section-title" data-notes="Now let's look at how deepfakes are actually being used in the real world — the good, the bad, and the deeply troubling. These applications show how synthetic media is used to manipulate, deceive, and erode trust at scale.">
    <h2>Deepfake <span class="orange">Applications</span></h2>
    <div class="divider" style="background:var(--orange)"></div>
  </div>

  <div class="slide" data-notes="The three main categories of deepfake misuse: political disinformation, where deepfakes are used to spread false information; non-consensual pornography, which is the most prevalent form; and fraud/scams. These often overlap.">
    <h2>Politics, Disinformation &amp; Porn</h2>
    <img src="img/slide47_Picture_6.jpg" class="medium">
  </div>

  <div class="slide" data-notes="A more complex case: memorial deepfakes. Companies now offer to 'resurrect' deceased loved ones as interactive AI avatars. The grandma example here shows a tablet-based avatar that family members can talk to. Is this comforting or disturbing? Who has the right to decide? Ask students for their reactions.">
    <h2>Memorial Deepfakes</h2>
    <div class="two-col" style="max-width:950px; align-items:start">
      <div>
        <img src="img/slide48_Picture_7.png" class="medium">
      </div>
      <div>
        <video src="img/slide48_Silicon-Intelligence-tablet-and-avatar-of-grandma-talking.mp4" controls style="width:100%; border-radius:8px"></video>
      </div>
    </div>
  </div>

  <div class="slide" data-notes="An even more provocative version — memorial deepfakes of saints and religious figures. This raises questions about cultural sensitivity, religious appropriation, and who has the authority to 'resurrect' historical or sacred figures.">
    <h2>Memorial Deepfakes <span class="dim">(Saints Version)</span></h2>
    <video src="img/slide49_ScreenRecording_10-17-2025_12-32-52_1.mp4" controls style="width:80%; max-width:700px; border-radius:8px"></video>
  </div>

  <div class="slide" data-notes="DeepFaceLab and Faceswap are the two main open-source face-swapping tools. DeepFaceLab is the power-user choice with extensive options. Faceswap is more beginner-friendly with a GUI. Both are free and on GitHub.">
    <h2>DeepFake Faces</h2>
    <div class="two-col" style="max-width:950px; align-items:start">
      <div>
        <iframe src="https://www.youtube-nocookie.com/embed/uAPUkgeiFVY?start=85" style="width:100%; aspect-ratio:16/9; border:none; border-radius:8px" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen></iframe>
        <video src="img/deepfake_faces_clip.mp4" controls style="width:100%; aspect-ratio:16/9; border-radius:8px; margin-top:12px"></video>
      </div>
      <div>
        <div class="card" style="margin-bottom:12px">
          <h3>DeepFaceLab</h3>
          <p><a href="https://github.com/iperov/DeepFaceLab" target="_blank">GitHub: iperov/DeepFaceLab</a></p>
          <p style="margin-top:6px">Wide variety of options, continually updated — the go-to choice for many users.</p>
        </div>
        <div class="card green-border">
          <h3 style="color:var(--green)">Faceswap</h3>
          <p><a href="https://github.com/deepfakes/faceswap" target="_blank">GitHub: deepfakes/faceswap</a></p>
          <p style="margin-top:6px">Friendly GUI and supportive community, accessible for beginners.</p>
        </div>
      </div>
    </div>
  </div>

  <div class="slide" data-notes="First Order Motion Model — takes a single portrait image and animates it using a driving video. You can make any photo move and talk like the person in the driving video. The key breakthrough was doing this without any face-specific training.">
    <h2>DeepFake Bodies</h2>
    <div class="two-col" style="max-width:950px; align-items:start">
      <div>
        <iframe src="https://www.youtube-nocookie.com/embed/PCBTZh41Ris?start=134" style="width:100%; aspect-ratio:16/9; border:none; border-radius:8px" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen></iframe>
      </div>
      <div class="card">
        <h3>First Order Model</h3>
        <p><a href="https://github.com/AliaksandrSiarohin/first-order-model" target="_blank">GitHub: AliaksandrSiarohin/first-order-model</a></p>
        <p style="margin-top:6px">Animates portrait images using a driving video.</p>
      </div>
    </div>
  </div>

  <div class="slide" data-notes="Voice synthesis has advanced rapidly. Tacotron/Tacotron2 was an early neural TTS model. Mozilla TTS made it open-source. VITS combines variational inference with adversarial training for high-quality speech. RVC allows voice cloning from just a few minutes of audio — this is what powers most deepfake voice content.">
    <h2>DeepFake Voices</h2>
    <div class="two-col" style="max-width:900px; align-items:start">
      <div class="card">
        <h3>Voice Synthesis</h3>
        <p style="color:var(--dim); margin-top:8px">Generate realistic speech from text, or clone a voice from a short audio sample</p>
      </div>
      <div>
        <div class="card" style="margin-bottom:12px">
          <h3>Popular Resources</h3>
          <ul>
            <li><strong>Tacotron / Tacotron2</strong> — Neural text-to-speech</li>
            <li><strong>Mozilla TTS</strong> — Open-source TTS engine</li>
            <li><strong>VITS</strong> — High-quality end-to-end TTS</li>
            <li><strong>RVC</strong> — Retrieval-based Voice Conversion</li>
          </ul>
        </div>
      </div>
    </div>
  </div>

  <div class="slide" data-notes="Deepfakes in entertainment. Kendrick Lamar's 'The Heart Part 5' music video used deepfake technology to transform his face into OJ Simpson, Kanye West, Jussie Smollett, Will Smith, Kobe Bryant, and Nipsey Hussle. Here the artist chose to use deepfakes with a clear artistic intent — commentary on Black celebrity and public image. Is this a legitimate use?">
    <h2>Entertainment</h2>
    <div class="two-col" style="max-width:950px; align-items:start">
      <div>
        <iframe src="https://www.youtube-nocookie.com/embed/9WfZuNceFDM?start=16" style="width:100%; aspect-ratio:16/9; border:none; border-radius:8px" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen></iframe>
      </div>
      <div>
        <video src="img/slide50_Online_Media_2.mp4" controls style="width:100%; border-radius:8px"></video>
      </div>
    </div>
  </div>

  <div class="slide" data-notes="Can deepfakes be a form of activism? These examples blur the line between commentary, satire, and manipulation. When does a deepfake go from being a powerful political statement to being disinformation? Ask students: where would they draw the line?">
    <h2>Activism?</h2>
    <div class="two-col" style="max-width:950px; align-items:start">
      <div>
        <iframe src="https://www.youtube-nocookie.com/embed/a95O4JsMBHQ" style="width:100%; aspect-ratio:16/9; border:none; border-radius:8px" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen></iframe>
      </div>
      <div>
        <video src="img/slide51_FuckKanye.mp4" controls style="width:100%; border-radius:8px"></video>
      </div>
    </div>
  </div>

  <div class="slide" data-notes="Deepfakes are heavily used in scams. Romance scams: fake video calls with AI-generated faces to build emotional connections and extract money. Crypto scams: deepfaked celebrities endorsing fake investment schemes. Both exploit trust and are increasingly hard to detect.">
    <h2>Scams</h2>
    <div class="two-col" style="max-width:900px; align-items:start">
      <div class="card red-border">
        <h3 style="color:var(--red)">Romance Scams</h3>
        <img src="img/slide52_Picture_10.jpg" style="margin-top:12px; width:100%; max-height:280px; object-fit:contain; border-radius:6px">
      </div>
      <div class="card yellow-border">
        <h3 style="color:var(--yellow)">Crypto Scams</h3>
        <img src="img/slide52_Picture_12.png" style="margin-top:12px; width:100%; max-height:280px; object-fit:contain; border-radius:6px">
      </div>
    </div>
  </div>

  <div class="slide" data-notes="AI companionship is a growing market — chatbots and avatars that provide emotional connection. Some use deepfake technology to create realistic virtual partners. This raises questions about dependency, manipulation, and what happens when the line between synthetic and real relationships blurs. The Luigi AI example went viral.">
    <h2>Companionship</h2>
    <div class="two-col" style="max-width:950px; align-items:start">
      <div>
        <img src="img/slide53_Picture_2.png" class="medium">
      </div>
      <div>
        <video src="img/slide53_ScreenRecording_10-14-2025_09-53-10_1.mp4" controls style="width:100%; border-radius:8px"></video>
      </div>
    </div>
  </div>

  <div class="slide" data-notes="AI-generated personas are also being used in sex work — virtual influencers and AI-generated models on platforms like OnlyFans. This displaces real workers while also creating content that can be indistinguishable from photos of real people.">
    <h2>Prostitution</h2>
    <img src="img/slide54_Picture_4.png" class="medium">
  </div>

  <!-- ============================================ -->
  <!-- DEGREES OF SIMULATION                         -->
  <!-- ============================================ -->

  <div class="slide section-title" data-notes="Now that we've seen how deepfakes are used in practice, let's introduce a conceptual framework. Not all deepfakes are the same — they exist on a spectrum of simulation. Understanding this spectrum helps us think more clearly about the different harms and uses.">
    <h2>Degrees of <span class="yellow">Simulation</span></h2>
    <div class="divider" style="background:var(--yellow)"></div>
  </div>

  <div class="slide" data-notes="First degree: the faithful copy. An AI reproduction that tries to be as accurate to reality as possible. Think of a deepfake that perfectly replicates how a person looks and speaks. The goal is indistinguishability. Play the video example.">
    <h2>Faithful Copy</h2>
    <iframe src="https://www.youtube-nocookie.com/embed/oQ7V74s6e04" style="width:80%; max-width:700px; aspect-ratio:16/9; border:none; border-radius:8px" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen></iframe>
  </div>

  <div class="slide" data-notes="Second degree: the distorted copy. The AI output is recognisably based on reality but deliberately or accidentally altered. This is where artistic deepfakes and glitchy outputs live. The distortion itself can be meaningful — or it can be a sign of the technology's limitations.">
    <h2>Distorted Copy</h2>
    <div class="img-row">
      <img src="img/slide66_Content_Placeholder_4.png" class="medium">
      <img src="img/slide66_Picture_4.png" class="medium">
    </div>
  </div>

  <div class="slide" data-notes="Third degree: masking the absence of reality. Creating a deepfake of something that never happened — like this fake video of Zelensky announcing Ukraine's surrender, which circulated on social media during the war. The deepfake doesn't copy reality; it fabricates a reality that doesn't exist. This is where disinformation lives.">
    <h2>Masking the Absence of Reality</h2>
    <div class="two-col" style="max-width:950px; align-items:start">
      <div>
        <video src="img/slide67_Deepfake_video_of_Volodymyr_Zelensky_surrendering_surfaces_on_social_media.mp4" controls style="width:100%; border-radius:8px"></video>
        <p class="caption">Deepfake of Zelensky "surrendering"</p>
      </div>
      <div>
        <img src="img/slide67_Picture_4.jpg" class="medium">
      </div>
    </div>
  </div>

  <div class="slide" data-notes="Fourth degree: a new reality entirely. When synthetic media becomes indistinguishable from reality at scale, the very concept of 'real' footage starts to lose meaning. We enter a world where anything can be faked, and therefore nothing can be fully trusted. This has profound implications for evidence, journalism, and trust.">
    <h2>A New Reality</h2>
    <div class="card" style="max-width:700px">
      <p style="font-size:1.5rem; color:var(--text); line-height:1.8">When synthetic media becomes indistinguishable from reality, deepfakes don't just <em>copy</em> the world — they create a <strong>new one</strong>.</p>
    </div>
  </div>

  <!-- ============================================ -->
  <!-- PART 11: CONSENT & HARM                      -->
  <!-- ============================================ -->

  <div class="slide section-title" data-notes="Now the hardest section. We need to talk about consent — specifically, how deepfake pornography violates consent at different levels. This is the most prevalent harm caused by deepfakes today.">
    <h2><span class="red">Porn</span> &amp; Consent</h2>
    <div class="divider" style="background:var(--red)"></div>
  </div>

  <div class="slide" data-notes="Two-party violation: a real person's face is swapped onto another real person's body in pornographic content. Neither party consented. This is the classic deepfake porn scenario — taking a celebrity's face and putting it on a porn performer's body. Click the image to unblur if needed for discussion.">
    <h2>Porn <span class="red">(Two Party Violation)</span></h2>
    <h3 style="color:var(--dim)">Faceswap / Bodyswap</h3>
    <img src="img/slide56_Picture_2.jpg" class="medium" style="filter:blur(8px); transition:filter 0.3s" onclick="this.style.filter=this.style.filter==='none'?'blur(8px)':'none'" title="Click to toggle blur">
    <p class="caption">Click image to toggle blur</p>
  </div>

  <div class="slide" data-notes="One-party violation with Deepnude: an AI that generates nude images from clothed photos. Only one person is involved — the victim whose clothed photo is 'undressed' by AI. The original Deepnude app was taken down but the code was open-sourced and countless clones exist. This disproportionately targets women and girls.">
    <h2>Porn <span class="red">(One Party Violation)</span></h2>
    <h3 style="color:var(--dim)">Deepnude</h3>
    <img src="img/slide57_Picture_2.jpg" class="medium" style="filter:blur(8px); transition:filter 0.3s" onclick="this.style.filter=this.style.filter==='none'?'blur(8px)':'none'" title="Click to toggle blur">
    <p class="caption">Click image to toggle blur</p>
  </div>

  <div class="slide" data-notes="Another form of one-party violation: generating explicit content purely from text prompts describing a real person. No source image needed — just a name and description. The 'waifu' variant generates anime-style explicit content based on real people. Both are violations even though no photograph was used as input.">
    <h2>Porn <span class="red">(One Party Violation)</span></h2>
    <div class="two-col" style="max-width:700px">
      <div class="card red-border">
        <h3 style="color:var(--red)">Text Prompt</h3>
        <p>Generating explicit content of real people via text descriptions</p>
      </div>
      <div class="card pink-border">
        <h3 style="color:var(--pink)">Text Prompt (Waifu)</h3>
        <p>Generating explicit anime-style content based on real people</p>
      </div>
    </div>
  </div>

  <div class="slide" data-notes="DignifAI was a counter-movement that used AI to add clothing to images of women in revealing outfits. While framed as 'dignifying' women, it was still non-consensual image manipulation — and was widely seen as a tool of misogynistic control, deciding for women what they should look like.">
    <h2>Porn <span class="red">(One Party Violation)</span></h2>
    <h3 style="color:var(--dim)">DignifAI</h3>
    <div class="img-row">
      <img src="img/slide59_Picture_2.jpg" class="medium">
      <img src="img/slide59_Picture_4.jpg" class="medium">
    </div>
  </div>

  <div class="slide" data-notes="An interesting edge case: zero-party violation. If the AI generates explicit content of a person who doesn't exist — purely synthetic — is anyone harmed? There's no victim whose likeness was stolen. But critics argue it normalises the creation of non-consensual content and the synthetic person may still resemble real people.">
    <h2>Porn <span class="yellow">(Zero Party Violation?)</span></h2>
    <img src="img/slide60_Picture_2.png" class="medium">
  </div>

  <div class="slide" data-notes="The age manipulation case: CoconutKitty was a real influencer who used AI to de-age herself, creating a version that looked like a teenager. This raises profound questions about virtual CSAM — if AI makes an adult look like a child in explicit content, is it child exploitation? Most legal systems say yes.">
    <h2>"Cosplay" Porn <span class="red">(Age)</span></h2>
    <div class="img-row">
      <img src="img/slide61_Picture_4.png" class="medium">
      <img src="img/slide61_Picture_7.png" class="medium">
    </div>
    <p class="caption">@CoconutKitty</p>
  </div>

  <div class="slide" data-notes="Perhaps the most disturbing case: AI-generated explicit content that simulates disabilities like Down syndrome. This fetishises and dehumanises people with disabilities, creating content that would be nearly impossible to produce otherwise. It reveals how generative AI can enable entirely new categories of exploitation.">
    <h2>"Cosplay" Porn <span class="red">(Down's)</span></h2>
    <img src="img/slide62_Picture_15.png" class="medium">
  </div>

  <div class="slide" data-notes="The takeaway from this entire section: any deepfake created without the subject's consent is an infringement — regardless of how 'harmless' it seems. Even the celebrity hug examples that seem innocent are using someone's likeness without permission. Consent is the red line.">
    <h2>No-Consent Deepfake = <span class="red">Infringement</span></h2>
    <div class="card red-border" style="max-width:500px; margin-bottom:24px">
      <p style="font-size:1.5rem; color:var(--text); text-align:center; line-height:1.6">no-consent deepfake<br><strong class="red">=</strong><br>infringement</p>
    </div>
    <video src="img/slide63_CelebrityHugs.mp4" controls style="width:60%; max-width:600px; border-radius:8px"></video>
  </div>


  <div class="slide" data-notes="A closing video to reflect on before opening discussion. Let it play fully — give students a moment to sit with it before moving to questions.">
    <video src="img/closing_video.mp4" controls style="max-height:70vh; max-width:80%; border-radius:8px"></video>
  </div>

  <div class="slide title-slide" data-notes="Open the floor for questions. Encourage students to share their reactions — what surprised them? What concerned them? What excited them? Remind them that understanding these tools is the first step to using them responsibly.">
    <h1>Questions<span class="accent">?</span> Comments<span class="accent">?</span></h1>
  </div>

  <div class="slide title-slide" data-notes="Thank you everyone! If you have more questions, feel free to reach out. Remember: these tools are powerful — use them with intention and respect for others.">
    <h1>Thank You<span class="pink">!</span></h1>
    <p class="subtitle">Questions?</p>
  </div>

  <div class="slide section-title" data-notes="Time for the hands-on workshop. We'll be exploring our own generative selves — experimenting with these tools first-hand so you can understand both their power and their limitations.">
    <h2><span class="green">Workshop</span></h2>
    <div class="divider" style="background:var(--green)"></div>
    <p style="color:var(--text); font-size:1.4rem; margin-top:16px">Owning Your Generative Self</p>
  </div>

  <!-- PART 7: GENAI TOOLS & RESOURCES              -->
  <!-- ============================================ -->

  <div class="slide section-title" data-notes="Now for the fun part — let's look at the actual tools available. I've organised these into four categories: image generation, enhancement, video generation, and 3D/immersive. We'll also touch on voice synthesis.">
    <h2>GenAI <span class="accent">Tools</span> &amp; Resources</h2>
    <div class="divider"></div>
  </div>

  <div class="slide" data-notes="Overview of generative AI resources and tools. The key modalities are text-to-image, text-to-video, image-to-image transformation, image-to-video animation, and combined image+text-to-video. Each has different tools and different levels of maturity.">
    <h2>Generative AI Resources</h2>
    <h3>Image &amp; Video Synthesis</h3>
    <div class="three-col" style="max-width:800px; margin-top:16px">
      <div class="card"><p style="color:var(--text)">Text-to-Image</p></div>
      <div class="card"><p style="color:var(--text)">Text-to-Video</p></div>
      <div class="card"><p style="color:var(--text)">Image-to-Image</p></div>
      <div class="card"><p style="color:var(--text)">Image-to-Video</p></div>
      <div class="card" style="grid-column: span 2"><p style="color:var(--text)">Image+Text-to-Video</p></div>
    </div>
  </div>

  <!-- ——— SUB-GROUP: IMAGE GENERATION ——— -->

  <div class="slide sub-section" data-notes="Let's start with image generation — the most mature category of generative AI tools.">
    <h3>Image Generation</h3>
    <div class="sub-divider"></div>
  </div>

  <div class="slide" data-notes="DeepFaceLab and Faceswap are the two main open-source face-swapping tools. DeepFaceLab is the power-user choice with extensive options. Faceswap is more beginner-friendly with a GUI. Both are free and on GitHub. Play the video to show a face-swap example.">
    <h2>DeepFake Faces</h2>
    <div class="two-col" style="max-width:950px; align-items:start">
      <div>
        <iframe src="https://www.youtube-nocookie.com/embed/uAPUkgeiFVY?start=85" style="width:100%; aspect-ratio:16/9; border:none; border-radius:8px" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen></iframe>
      </div>
      <div>
        <div class="card" style="margin-bottom:12px">
          <h3>DeepFaceLab</h3>
          <p><a href="https://github.com/iperov/DeepFaceLab" target="_blank">GitHub: iperov/DeepFaceLab</a></p>
          <p style="margin-top:6px">Wide variety of options, continually updated — the go-to choice for many users.</p>
        </div>
        <div class="card green-border">
          <h3 style="color:var(--green)">Faceswap</h3>
          <p><a href="https://github.com/deepfakes/faceswap" target="_blank">GitHub: deepfakes/faceswap</a></p>
          <p style="margin-top:6px">Friendly GUI and supportive community, accessible for beginners.</p>
        </div>
      </div>
    </div>
  </div>

  <div class="slide" data-notes="First Order Motion Model — this takes a single portrait image and animates it using a driving video. So you can make any photo move and talk like the person in the driving video. The key breakthrough was doing this without any face-specific training.">
    <h2>DeepFake Bodies</h2>
    <div class="two-col" style="max-width:950px; align-items:start">
      <div>
        <iframe src="https://www.youtube-nocookie.com/embed/PCBTZh41Ris?start=134" style="width:100%; aspect-ratio:16/9; border:none; border-radius:8px" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen></iframe>
      </div>
      <div class="card">
        <h3>First Order Model</h3>
        <p><a href="https://github.com/AliaksandrSiarohin/first-order-model" target="_blank">GitHub: AliaksandrSiarohin/first-order-model</a></p>
        <p style="margin-top:6px">Animates portrait images using a driving video.</p>
      </div>
    </div>
  </div>

  <div class="slide" data-notes="These are screenshots of the GitHub repositories and community resources for deepfake face generation. Notice how open and accessible these tools are — anyone with a computer can use them.">
    <h2>Deepfakes for Faces — Repositories</h2>
    <div class="img-row">
      <img src="img/slide23_Picture_13.png" class="medium">
      <img src="img/slide23_Picture_15.png" class="medium">
    </div>
  </div>

  <div class="slide" data-notes="Stable Diffusion is the landmark open-source image generation model. Free to use, runs locally on modest hardware (4GB VRAM GPU). Trained on LAION — remember the copyright and CSAM issues we discussed. Training cost around 600,000 USD on 256 NVIDIA A100 GPUs. Play the demo video.">
    <h2>Stable Diffusion</h2>
    <div class="two-col" style="max-width:950px; align-items:start">
      <div class="card">
        <h3>Open Source Text-to-Image</h3>
        <p><a href="https://stability.ai/" target="_blank">stability.ai</a> · <a href="https://stablediffusionweb.com" target="_blank">stablediffusionweb.com</a></p>
        <ul style="margin-top:12px">
          <li><strong class="green">Free &amp; Open Source</strong></li>
          <li>Text-to-Image, Image-to-Image, Image-to-Video</li>
          <li>Runs on GPU with 4GB VRAM</li>
          <li>Claims no rights on generated images</li>
          <li>Trained on LAION dataset</li>
          <li>Trained on 256 NVIDIA A100 GPUs — 150,000 GPU hours (~$600,000 USD)</li>
        </ul>
      </div>
      <div>
        <video src="img/slide24_RPReplay_Final1713497286.mov" controls style="width:100%; border-radius:8px"></video>
      </div>
    </div>
  </div>

  <div class="slide" data-notes="PhotoMaker uses stacked ID embeddings to generate consistent characters across multiple images. Very useful for creating a character that looks the same across different scenes and poses — a major challenge in generative AI. Free on HuggingFace.">
    <h2>PhotoMaker: Consistent Characters</h2>
    <div class="two-col" style="max-width:950px; align-items:start">
      <div class="card">
        <h3>Stacked ID Embedding</h3>
        <p><a href="https://huggingface.co/spaces/TencentARC/PhotoMaker" target="_blank">HuggingFace Demo</a></p>
        <ul style="margin-top:8px">
          <li><strong class="green">Free</strong></li>
          <li>Great for consistent character generation</li>
        </ul>
      </div>
      <div>
        <video src="img/slide36_photomaker_demo.mp4" controls style="width:100%; border-radius:8px"></video>
      </div>
    </div>
  </div>

  <!-- ——— SUB-GROUP: ENHANCEMENT ——— -->

  <div class="slide sub-section" data-notes="Next category: enhancement tools. These take existing images or video and improve them — upscaling resolution, sharpening, inpainting missing areas, or generating in real time.">
    <h3>Enhancement</h3>
    <div class="sub-divider"></div>
  </div>

  <div class="slide" data-notes="Magnific uses AI to upscale images — not just making them bigger, but actually hallucinating new detail. Free tier available. Great for taking low-res source material and making it print-ready.">
    <h2>Magnific: Image Enhancement</h2>
    <div class="two-col" style="max-width:950px; align-items:start">
      <div class="card">
        <h3>AI Upscaling</h3>
        <p><a href="https://magnific.ai/" target="_blank">magnific.ai</a></p>
        <ul style="margin-top:8px">
          <li><strong class="green">Free</strong></li>
          <li>Increases size and resolution of images</li>
        </ul>
      </div>
      <div>
        <img src="img/slide25_Picture_7.png" class="medium">
      </div>
    </div>
  </div>

  <div class="slide" data-notes="Topaz is the professional-grade option for both photo and video enhancement. Paid software — 199 for photos, 299 for the full AI suite. Used widely in film post-production and photography.">
    <h2>Topaz: Image + Video Enhancement</h2>
    <div class="two-col" style="max-width:950px; align-items:start">
      <div class="card">
        <h3>Professional Enhancement</h3>
        <p><a href="https://www.topazlabs.com/" target="_blank">topazlabs.com</a></p>
        <ul style="margin-top:8px">
          <li>Photo Enhancement: <strong>$199</strong></li>
          <li>AI Enhancement: <strong>$299</strong></li>
        </ul>
      </div>
      <div>
        <iframe src="https://www.youtube-nocookie.com/embed/vC01CNmhusU?start=14" style="width:100%; aspect-ratio:16/9; border:none; border-radius:8px" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen></iframe>
      </div>
    </div>
  </div>

  <div class="slide" data-notes="KREA is fascinating because it generates images in real time as you draw. You sketch rough shapes and it turns them into photorealistic images instantly. Also has an image enhancer. Play the demo video.">
    <h2>KREA: Real-Time Generation</h2>
    <div class="two-col" style="max-width:950px; align-items:start">
      <div class="card">
        <h3>Real-Time AI</h3>
        <p><a href="https://www.krea.ai/" target="_blank">krea.ai</a></p>
        <ul style="margin-top:8px">
          <li>Real-time image generation from illustrations</li>
          <li>Image Enhancer</li>
        </ul>
      </div>
      <div>
        <iframe src="https://www.youtube-nocookie.com/embed/tCtshypObhw" style="width:100%; aspect-ratio:16/9; border:none; border-radius:8px" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen></iframe>
      </div>
    </div>
  </div>

  <div class="slide" data-notes="Leonardo AI offers real-time inpainting — you can paint over parts of an image and have the AI fill them in with context-aware content. Free to use. Good for quick edits and creative experimentation.">
    <h2>Leonardo AI: Real-Time Inpainting</h2>
    <div class="two-col" style="max-width:950px; align-items:start">
      <div class="card">
        <h3>Inpainting Tool</h3>
        <p><a href="https://app.leonardo.ai/" target="_blank">app.leonardo.ai</a></p>
        <p style="margin-top:8px"><strong class="green">Free</strong></p>
      </div>
      <div>
        <iframe src="https://www.youtube-nocookie.com/embed/0m2gYAbUi7M?start=817" style="width:100%; aspect-ratio:16/9; border:none; border-radius:8px" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen></iframe>
      </div>
    </div>
  </div>

  <!-- ——— SUB-GROUP: VIDEO GENERATION ——— -->

  <div class="slide sub-section" data-notes="Now video generation — the fastest-moving area in generative AI right now. These tools are improving at a staggering rate.">
    <h3>Video Generation</h3>
    <div class="sub-divider"></div>
  </div>

  <div class="slide" data-notes="OpenAI's SORA was the bombshell announcement in early 2024 — a text-to-video model that produces remarkably coherent, cinematic video from text prompts. It models video as a 'world simulator', understanding physics, lighting, and object permanence. Play the demo.">
    <h2>OpenAI's SORA: Text-to-Video</h2>
    <div class="card" style="max-width:700px; margin-bottom:16px">
      <p><a href="https://openai.com/research/video-generation-models-as-world-simulators" target="_blank">Research Paper</a> · <a href="https://openai.com/sora#capabilities" target="_blank">Capabilities</a></p>
    </div>
    <iframe src="https://www.youtube-nocookie.com/embed/2fAPgOCjToA" style="width:80%; max-width:700px; aspect-ratio:16/9; border:none; border-radius:8px" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen></iframe>
  </div>

  <div class="slide" data-notes="Haiper generates short 2-second video clips that are among the most realistic available. Free to use. Supports both text-to-video and image-to-video. Good for quick prototyping and social media content.">
    <h2>Haiper: Video Generation</h2>
    <div class="two-col" style="max-width:950px; align-items:start">
      <div class="card">
        <h3>2-Second Video Gen</h3>
        <p><a href="https://haiper.ai/" target="_blank">haiper.ai</a></p>
        <ul style="margin-top:8px">
          <li><strong class="green">Free</strong></li>
          <li>Text-to-Video, Image-to-Video</li>
          <li>Among the most realistic video generation</li>
        </ul>
      </div>
      <div>
        <iframe src="https://www.youtube-nocookie.com/embed/fJQbP34GoHQ" style="width:100%; aspect-ratio:16/9; border:none; border-radius:8px" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen></iframe>
      </div>
    </div>
  </div>

  <div class="slide" data-notes="Pika Labs specialises in adding AI-powered effects to video — their PixAdditions feature lets you add or modify elements in existing video using text prompts. Free tier available. Play the demo video.">
    <h2>Pika Labs: PixAdditions</h2>
    <div class="two-col" style="max-width:950px; align-items:start">
      <div class="card">
        <h3>AI Video Effects</h3>
        <p><a href="https://pika.art/" target="_blank">pika.art</a></p>
        <ul style="margin-top:8px">
          <li><strong class="green">Free</strong></li>
          <li>Prompts: Text &amp; Images</li>
        </ul>
      </div>
      <div>
        <video src="img/slide33_Video_4.mp4" controls style="width:100%; border-radius:8px"></video>
      </div>
    </div>
  </div>

  <div class="slide" data-notes="LTX Studio is designed for storyboarding workflows — it generates both images and short video clips that maintain visual consistency across scenes. Very useful for pre-production in film and animation.">
    <h2>LTX Studio: Storyboarding</h2>
    <div class="two-col" style="max-width:950px; align-items:start">
      <div class="card">
        <h3>Generative Storyboarding</h3>
        <p><a href="https://ltx.studio/" target="_blank">ltx.studio</a></p>
        <p style="margin-top:8px">Generate images + videos for storyboarding workflows</p>
      </div>
      <div>
        <iframe src="https://www.youtube-nocookie.com/embed/CFGI0wflYvA" style="width:100%; aspect-ratio:16/9; border:none; border-radius:8px" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen></iframe>
      </div>
    </div>
  </div>

  <div class="slide" data-notes="Mootion bridges the gap between motion capture and video generation. You can go from text to motion, motion to video, or extract motion from existing video. Pay-per-credit model. Useful for animation, gaming, and virtual production.">
    <h2>Mootion: AI Motion</h2>
    <div class="two-col" style="max-width:950px; align-items:start">
      <div class="card">
        <h3>Motion Capture &amp; Generation</h3>
        <p><a href="https://www.mootion.com/" target="_blank">mootion.com</a></p>
        <ul style="margin-top:8px">
          <li>Pay per Credit</li>
          <li>Text-to-Motion</li>
          <li>Motion-to-Video</li>
          <li>Video-to-Motion</li>
        </ul>
      </div>
      <div>
        <video src="img/slide37_motion-to-video1.mov" controls style="width:100%; border-radius:8px"></video>
      </div>
    </div>
  </div>

  <!-- ——— SUB-GROUP: 3D & IMMERSIVE ——— -->

  <div class="slide sub-section" data-notes="Finally, 3D and immersive content generation — where generative AI meets spatial computing and VR.">
    <h3>3D &amp; Immersive</h3>
    <div class="sub-divider"></div>
  </div>

  <div class="slide" data-notes="InseRF is a research project that inserts AI-generated objects into 3D Neural Radiance Fields — essentially placing synthetic objects into photorealistic 3D scenes. No consumer app yet, but the implications for VR and film are huge.">
    <h2>InseRF: 3D Image Generation</h2>
    <div class="two-col" style="max-width:950px; align-items:start">
      <div class="card">
        <h3>Neural Radiance Fields</h3>
        <p style="color:var(--dim)">No app or website yet — research project</p>
      </div>
      <div>
        <iframe src="https://www.youtube-nocookie.com/embed/OsmAwajM6_E" style="width:100%; aspect-ratio:16/9; border:none; border-radius:8px" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen></iframe>
      </div>
    </div>
  </div>

  <div class="slide" data-notes="FPRF applies photorealistic style transfer to 3D Neural Radiance Fields. You can take a 3D scene and apply a completely different artistic style while maintaining the 3D structure. This video shows the flower scene rendered with different styles.">
    <h2>FPRF: 3D Style Transfer</h2>
    <p style="margin-bottom:16px; font-size:1.15rem; color:var(--dim)">Feed-Forward Photorealistic Style Transfer of Large-Scale 3D Neural Radiance Fields</p>
    <video src="img/slide35_LLFF_flower.mp4" controls autoplay muted loop style="width:70%; max-width:600px; border-radius:8px"></video>
  </div>

  <div class="slide" data-notes="A creative hack with Midjourney: by using equirectangular projection prompts, you can generate 360-degree panoramic images suitable for VR environments. The key is the prompt engineering — specifying equirectangular projection and ultra-wide angle. Read the prompt aloud.">
    <h2>Midjourney: 360 VR Environments</h2>
    <div class="card" style="max-width:800px; margin-bottom:16px">
      <p style="font-family:var(--mono); font-size:1.05rem; color:var(--accent); line-height:1.7">/imagine prompt: <strong>equirectangular projection of</strong> a visually stunning landscape: majestic mountains, golden sunset, <strong>expansive, awe-inspiring, breathtaking, vivid colors, dramatic lighting, sharp focus, good exposure, insanely detailed, ultra-wide angle lens</strong> --no black edges, text, any distortion --ar 16:9 --v 4 --style 4c</p>
    </div>
    <iframe src="https://www.youtube-nocookie.com/embed/F4wGqCoFIDg" style="width:80%; max-width:700px; aspect-ratio:16/9; border:none; border-radius:8px" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen></iframe>
  </div>

  <!-- ——— INTERACTIVE WORKSHOP DESCRIPTION ——— -->

  <div class="slide section-title" data-notes="Now it's your turn. We have a custom-built interactive workshop where you can experiment hands-on with 8 different generative AI tools — all running in your browser, powered by real AI models via the Replicate API. No accounts needed, no installs. Just open the link and start creating.">
    <h2>Hands-On <span class="green">Workshop</span></h2>
    <div class="divider" style="background:var(--green)"></div>
    <p style="color:var(--text); font-size:1.4rem; margin-top:16px">8 interactive tools — try them yourself</p>
  </div>

  <div class="slide" data-notes="Important framing for students: every model in this workshop is free, open-source, and independent — built by researchers, small teams, and open communities, not by Big Tech. FLUX-schnell is by Black Forest Labs (small German startup). SDXL is by Stability AI (open-weights). BLIP is by Salesforce Research (open-source). PhotoMaker is by TencentARC (academic research team). Hunyuan3D is by Tencent (open-source release). Shap-E was open-sourced by OpenAI but runs independently. The face swap and pose models are community-built. None of these require accounts with Google, Meta, OpenAI, or Anthropic. The models run on Replicate — a platform that hosts open-source models on shared GPU infrastructure, meaning we're not spinning up dedicated data centres. This is a deliberate choice: after spending an entire lecture on the empire of AI — resource extraction, labor exploitation, environmental cost — it would be contradictory to then hand students tools from those same empires. Open-source models represent a different path: transparent, community-driven, and with a lighter footprint. It's not perfect — GPU compute still has environmental costs — but it's a more conscious and ethical way to explore these technologies.">
    <h2>Workshop Tools</h2>
    <p style="color:var(--dim); margin-bottom:16px; font-size:1rem">All models are <strong style="color:var(--green)">free &amp; open-source</strong> — built by independent researchers, not Big Tech</p>
    <div style="display:grid; grid-template-columns:1fr 1fr 1fr 1fr; gap:10px; max-width:1050px; width:100%">
      <div class="card red-border" style="padding:14px 16px">
        <h3 style="color:var(--red); font-size:1.05rem">1. Text to Image</h3>
        <p style="font-size:0.95rem">Type a prompt, get an image</p>
        <p style="font-size:0.8rem; color:var(--dim); margin-top:6px">FLUX-schnell · ~5s</p>
      </div>
      <div class="card orange-border" style="padding:14px 16px">
        <h3 style="color:var(--orange); font-size:1.05rem">2. Image to Image</h3>
        <p style="font-size:0.95rem">Transform photos with text</p>
        <p style="font-size:0.8rem; color:var(--dim); margin-top:6px">SDXL · ~15s</p>
      </div>
      <div class="card yellow-border" style="padding:14px 16px">
        <h3 style="color:var(--yellow); font-size:1.05rem">3. Image to Text</h3>
        <p style="font-size:0.95rem">AI describes your image</p>
        <p style="font-size:0.8rem; color:var(--dim); margin-top:6px">BLIP · ~10s</p>
      </div>
      <div class="card pink-border" style="padding:14px 16px">
        <h3 style="color:var(--pink); font-size:1.05rem">4. PhotoMaker</h3>
        <p style="font-size:0.95rem">Your face in any style</p>
        <p style="font-size:0.8rem; color:var(--dim); margin-top:6px">TencentARC · ~30s</p>
      </div>
      <div class="card green-border" style="padding:14px 16px">
        <h3 style="color:var(--green); font-size:1.05rem">5. Image to 3D</h3>
        <p style="font-size:0.95rem">Photo → 3D model</p>
        <p style="font-size:0.8rem; color:var(--dim); margin-top:6px">Hunyuan3D · ~3min</p>
      </div>
      <div class="card" style="padding:14px 16px; border-left-color:var(--accent)">
        <h3 style="color:var(--accent); font-size:1.05rem">6. Text to 3D</h3>
        <p style="font-size:0.95rem">Describe → 3D model</p>
        <p style="font-size:0.8rem; color:var(--dim); margin-top:6px">Shap-E · ~2min</p>
      </div>
      <div class="card red-border" style="padding:14px 16px">
        <h3 style="color:var(--red); font-size:1.05rem">7. Face Swap</h3>
        <p style="font-size:0.95rem">Swap faces between photos</p>
        <p style="font-size:0.8rem; color:var(--dim); margin-top:6px">CodePlugTech · ~30s</p>
      </div>
      <div class="card" style="padding:14px 16px; border-left-color:var(--dim)">
        <h3 style="font-size:1.05rem">8. Pose Transfer</h3>
        <p style="font-size:0.95rem">Copy a pose to new image</p>
        <p style="font-size:0.8rem; color:var(--dim); margin-top:6px">ControlNet · ~20s</p>
      </div>
    </div>
    <p style="color:var(--dim); margin-top:14px; font-size:0.85rem">No Big Tech accounts needed · Shared GPU infrastructure · Lower environmental footprint</p>
  </div>

  <!-- ============================================ -->

  <!-- NAV -->
  <div id="slide-counter"></div>
  <div id="nav">
    <button onclick="prev()" title="Previous">&#8592;</button>
    <button onclick="next()" title="Next">&#8594;</button>
    <button onclick="toggleFullscreen()" title="Fullscreen (F)" style="font-size:0.8rem">&#x26F6;</button>
  </div>

  <script>
    const slides = document.querySelectorAll('.slide');
    const total = slides.length;
    let current = 0;
    let transitioning = false;

    function show(n) {
      n = Math.max(0, Math.min(n, total - 1));
      if (n === current || transitioning) return;

      transitioning = true;
      const prevSlide = slides[current];
      const nextSlide = slides[n];
      const prevIndex = current;

      prevSlide.classList.add('fade-out');
      prevSlide.addEventListener('animationend', function handler() {
        prevSlide.removeEventListener('animationend', handler);
        prevSlide.classList.remove('active', 'fade-out');
        current = n;
        var frags = nextSlide.querySelectorAll('.fragment');
        frags.forEach(function(f) {
          if (n < prevIndex) f.classList.add('visible');
          else f.classList.remove('visible');
        });
        nextSlide.classList.add('active');
        document.getElementById('slide-counter').textContent = (current + 1) + ' / ' + total;
        document.getElementById('progress').style.width = ((current + 1) / total * 100) + '%';
        transitioning = false;
      }, { once: true });

      setTimeout(function() {
        if (transitioning) {
          prevSlide.classList.remove('active', 'fade-out');
          current = n;
          var frags = nextSlide.querySelectorAll('.fragment');
          frags.forEach(function(f) {
            if (n < prevIndex) f.classList.add('visible');
            else f.classList.remove('visible');
          });
          nextSlide.classList.add('active');
          document.getElementById('slide-counter').textContent = (current + 1) + ' / ' + total;
          document.getElementById('progress').style.width = ((current + 1) / total * 100) + '%';
          transitioning = false;
        }
      }, 400);
    }

    function next() {
      var frags = slides[current].querySelectorAll('.fragment:not(.visible)');
      if (frags.length > 0) { frags[0].classList.add('visible'); return; }
      show(current + 1);
    }

    function prev() {
      var frags = slides[current].querySelectorAll('.fragment.visible');
      if (frags.length > 0) { frags[frags.length - 1].classList.remove('visible'); return; }
      show(current - 1);
    }

    function toggleFullscreen() {
      if (!document.fullscreenElement) document.documentElement.requestFullscreen().catch(function(){});
      else document.exitFullscreen();
    }

    // Presenter notes
    var presenterWin = null;
    var presenterStartTime = null;
    var presenterTimerInterval = null;

    function formatTime(ms) {
      var s = Math.floor(ms / 1000), m = Math.floor(s / 60), h = Math.floor(m / 60);
      s = s % 60; m = m % 60;
      return (h > 0 ? h + ':' : '') + (m < 10 ? '0' : '') + m + ':' + (s < 10 ? '0' : '') + s;
    }

    function updatePresenter() {
      if (!presenterWin || presenterWin.closed) return;
      var doc = presenterWin.document;
      var notes = slides[current].getAttribute('data-notes') || '(no notes for this slide)';
      var title = slides[current].querySelector('h1, h2');
      var titleText = title ? title.textContent : 'Slide ' + (current + 1);
      var nextTitle = '(end)';
      if (current + 1 < total) {
        var nt = slides[current + 1].querySelector('h1, h2');
        nextTitle = nt ? nt.textContent : 'Slide ' + (current + 2);
      }
      var el = doc.getElementById('p-slide-num'); if (el) el.textContent = 'Slide ' + (current + 1) + ' / ' + total;
      el = doc.getElementById('p-title'); if (el) el.textContent = titleText;
      el = doc.getElementById('p-notes'); if (el) el.textContent = notes;
      el = doc.getElementById('p-next'); if (el) el.textContent = nextTitle;
    }

    function openPresenter() {
      if (presenterWin && !presenterWin.closed) { presenterWin.focus(); return; }
      presenterWin = window.open('', 'presenter', 'width=500,height=400,toolbar=no,menubar=no');
      var doc = presenterWin.document;
      doc.open();
      doc.write('<!DOCTYPE html><html><head><title>Presenter Notes</title><style>body{background:#111;color:#e8e8e8;font-family:-apple-system,sans-serif;padding:20px;margin:0}.timer{font-size:3rem;font-family:monospace;color:#4a9eff;margin-bottom:8px}.slide-num{font-size:0.85rem;color:#777;margin-bottom:16px}.title{font-size:1.3rem;color:#fff;margin-bottom:16px;border-bottom:1px solid #333;padding-bottom:12px}.notes{font-size:1.1rem;line-height:1.8;color:#ccc;margin-bottom:24px;min-height:80px}.next-label{font-size:0.75rem;color:#777;margin-bottom:4px}.next-title{font-size:1rem;color:#999}button{padding:6px 16px;border:1px solid #444;background:transparent;color:#999;border-radius:4px;cursor:pointer;font-size:0.8rem;margin-right:8px}button:hover{border-color:#4a9eff;color:#4a9eff}.controls{margin-bottom:16px}</style></head><body><div class="timer" id="p-timer">00:00</div><div class="controls"><button id="p-reset">Reset Timer</button><button id="p-prev">&larr; Prev</button><button id="p-next-btn">Next &rarr;</button></div><div class="slide-num" id="p-slide-num"></div><div class="title" id="p-title"></div><div class="notes" id="p-notes"></div><div class="next-label">NEXT SLIDE</div><div class="next-title" id="p-next"></div></body></html>');
      doc.close();
      doc.getElementById('p-prev').addEventListener('click', function() { prev(); });
      doc.getElementById('p-next-btn').addEventListener('click', function() { next(); });
      doc.getElementById('p-reset').addEventListener('click', function() { presenterStartTime = Date.now(); });
      doc.addEventListener('keydown', function(e) {
        if (e.key === 'ArrowRight' || e.key === ' ') { e.preventDefault(); next(); }
        if (e.key === 'ArrowLeft') { e.preventDefault(); prev(); }
      });
      presenterStartTime = presenterStartTime || Date.now();
      clearInterval(presenterTimerInterval);
      presenterTimerInterval = setInterval(function() {
        if (presenterWin && !presenterWin.closed) {
          var el = presenterWin.document.getElementById('p-timer');
          if (el) el.textContent = formatTime(Date.now() - presenterStartTime);
        } else clearInterval(presenterTimerInterval);
      }, 1000);
      updatePresenter();
    }

    // Section navigation bar
    var sectionNav = document.getElementById('section-nav');
    var sectionSlides = [];
    slides.forEach(function(s, i) {
      if (s.classList.contains('section-title') || s.classList.contains('title-slide')) {
        var h = s.querySelector('h2') || s.querySelector('h1');
        if (h) {
          var label = h.textContent.trim();
          sectionSlides.push({ index: i, label: label });
          var btn = document.createElement('button');
          btn.textContent = label;
          btn.setAttribute('data-slide', i);
          btn.addEventListener('click', function() { show(i); });
          sectionNav.appendChild(btn);
        }
      }
    });

    function updateSectionNav() {
      var btns = sectionNav.querySelectorAll('button');
      var activeSection = 0;
      for (var i = sectionSlides.length - 1; i >= 0; i--) {
        if (current >= sectionSlides[i].index) { activeSection = i; break; }
      }
      btns.forEach(function(b, idx) {
        b.classList.toggle('active', idx === activeSection);
      });
    }

    // Patch show/next/prev to update section nav
    var __origShow = show;
    show = function(n) { __origShow(n); setTimeout(function() { updateSectionNav(); updatePresenter(); }, 50); };
    var __origNext = next;
    next = function() { __origNext(); setTimeout(function() { updateSectionNav(); updatePresenter(); }, 50); };
    var __origPrev = prev;
    prev = function() { __origPrev(); setTimeout(function() { updateSectionNav(); updatePresenter(); }, 50); };

    document.querySelector('#nav button:nth-child(1)').onclick = function() { prev(); };
    document.querySelector('#nav button:nth-child(2)').onclick = function() { next(); };

    document.addEventListener('keydown', function(e) {
      if (e.key === 'ArrowRight' || e.key === ' ' || e.key === 'Enter') { e.preventDefault(); next(); }
      if (e.key === 'ArrowLeft' || e.key === 'Backspace') { e.preventDefault(); prev(); }
      if (e.key === 'Home') { e.preventDefault(); show(0); }
      if (e.key === 'End') { e.preventDefault(); show(total - 1); }
      if (e.key === 'p' || e.key === 'P') openPresenter();
      if (e.key === 'f' || e.key === 'F') toggleFullscreen();
      if (e.key === 'n' || e.key === 'N') sectionNav.classList.toggle('visible');
    });

    var touchStartX = 0;
    document.addEventListener('touchstart', function(e) { touchStartX = e.touches[0].clientX; });
    document.addEventListener('touchend', function(e) {
      var diff = e.changedTouches[0].clientX - touchStartX;
      if (Math.abs(diff) > 50) { diff < 0 ? next() : prev(); }
    });

    updateSectionNav();
    show(0);
  </script>

</body>
</html>

