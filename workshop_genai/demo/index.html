<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>Generative AI Workshop</title>
  <link rel="stylesheet" href="css/style.css">

  <!-- TensorFlow.js -->
  <script src="https://cdn.jsdelivr.net/npm/@tensorflow/tfjs@4.17.0/dist/tf.min.js"></script>
  <!-- Magenta.js (Arbitrary Style Transfer) -->
  <script src="https://cdn.jsdelivr.net/npm/@magenta/image@0.2.1/dist/magentaimage.js"></script>
</head>
<body>
  <!-- Loading overlay -->
  <div id="loading-overlay">
    <div class="loader-content">
      <div class="spinner"></div>
      <p>Loading<span class="dots"></span></p>
      <p class="loader-sub">Preparing workshop materials</p>
    </div>
  </div>

  <!-- Header -->
  <header id="app-header">
    <h1>Generative AI Workshop</h1>
    <p class="subtitle">Explore generative AI models directly in your browser.</p>
  </header>

  <!-- Tab navigation -->
  <nav id="step-nav">
    <button class="step-tab active" data-step="1">Text to Image</button>
    <button class="step-tab" data-step="2">Image to Image</button>
    <button class="step-tab" data-step="3">Image to Text</button>
  </nav>

  <!-- ============ Tab 1: Text to Image ============ -->
  <section id="step-1" class="step active">
    <div class="step-intro">
      <h2>Text to Image</h2>
      <span class="step-badge">Text → Image</span>
      <p>Type a text prompt and a Stable Diffusion model will generate an image. No login or API key required.</p>
    </div>

    <div class="explainer-block" style="border-left-color:#c084fc">
      <h3 style="color:#c084fc">How it works</h3>
      <p><strong>Stable Diffusion</strong> is a <em>latent diffusion model</em> that starts from random noise and gradually removes it, guided by your text prompt. It uses a <strong>CLIP text encoder</strong> to understand the prompt, a <strong>UNet</strong> to denoise, and a <strong>VAE decoder</strong> to produce the final image. Generation takes 10–30 seconds.</p>
    </div>

    <textarea class="text-input" id="txt2img-prompt" placeholder="Describe the image you want to generate...">a photo of a cat wearing a tiny hat</textarea>

    <div class="loading-bar" id="txt2img-loading">
      <div class="loading-label">
        <span id="txt2img-loading-text">Generating image<span class="loading-dots"></span></span>
      </div>
      <div class="loading-track"><div class="loading-fill" id="txt2img-loading-fill"></div></div>
      <div class="loading-steps" id="txt2img-loading-status"></div>
    </div>

    <button class="btn-primary" id="btn-generate">Generate Image</button>

    <div id="txt2img-results" class="results hidden">
      <h3 style="text-align:center; font-family:var(--mono); color:var(--text-dim); margin-bottom:12px;">Generated Image</h3>
      <img id="img-generated" style="display:block; max-width:100%; border-radius:var(--radius); margin:0 auto;">
    </div>

    <div id="txt2img-error" class="results hidden">
      <div class="output-block mono" style="border-left-color:var(--red); color:var(--red);" id="txt2img-error-text"></div>
    </div>
  </section>

  <!-- ============ Tab 2: Image to Image (Style Transfer) ============ -->
  <section id="step-2" class="step">
    <div class="step-intro">
      <h2>Image to Image</h2>
      <span class="step-badge">Image → Image</span>
      <p>Upload a content image and a style image. The neural network will blend the style onto your content — all in your browser.</p>
    </div>

    <div class="explainer-block">
      <h3>How it works</h3>
      <p>A <strong>neural style transfer</strong> model separates the <em>content</em> (shapes, objects) from the <em>style</em> (colours, textures, brushstrokes) of two images, then recombines them. This uses the <strong>Magenta Arbitrary Style Transfer</strong> model (~10 MB) and runs entirely on your device.</p>
    </div>

    <div class="mirror-row">
      <div class="mirror-card">
        <h3>Content Image</h3>
        <div class="upload-area" id="upload-area-content">
          <label for="file-content" class="upload-label">
            <span class="upload-icon">+</span>
            <span>Upload content image</span>
          </label>
          <input type="file" id="file-content" accept="image/*">
        </div>
        <canvas id="canvas-content"></canvas>
      </div>
      <div class="mirror-card">
        <h3>Style Image</h3>
        <div class="upload-area" id="upload-area-style">
          <label for="file-style" class="upload-label">
            <span class="upload-icon">+</span>
            <span>Upload style image</span>
          </label>
          <input type="file" id="file-style" accept="image/*">
        </div>
        <canvas id="canvas-style"></canvas>
      </div>
    </div>

    <div class="loading-bar" id="img2img-loading">
      <div class="loading-label">
        <span>Loading style transfer model<span class="loading-dots"></span></span>
        <span id="img2img-loading-pct"></span>
      </div>
      <div class="loading-track"><div class="loading-fill" id="img2img-loading-fill"></div></div>
      <div class="loading-steps" id="img2img-loading-status"></div>
    </div>

    <button class="btn-primary" id="btn-stylize" disabled>Stylize</button>

    <div id="img2img-results" class="results hidden">
      <h3 style="text-align:center; font-family:var(--mono); color:var(--text-dim); margin-bottom:12px;">Result</h3>
      <canvas id="canvas-result" style="margin:0 auto;"></canvas>
    </div>
  </section>

  <!-- ============ Tab 3: Image to Text (Captioning) ============ -->
  <section id="step-3" class="step">
    <div class="step-intro">
      <h2>Image to Text</h2>
      <span class="step-badge">Image → Text</span>
      <p>Upload any image and a vision-language model will describe what it sees — generating a natural language caption from pixels.</p>
    </div>

    <div class="explainer-block" style="border-left-color:var(--green)">
      <h3 style="color:var(--green)">How it works</h3>
      <p>This uses <strong>BLIP (vit-gpt2)</strong>, a vision-language model combining a <em>Vision Transformer</em> (to understand the image) with <em>GPT-2</em> (to generate text). The ~300 MB model downloads on first use and runs entirely in your browser. Once cached, it loads instantly.</p>
    </div>

    <div class="upload-area" id="upload-area-caption">
      <label for="file-caption" class="upload-label">
        <span class="upload-icon">+</span>
        <span>Upload an image to caption</span>
      </label>
      <input type="file" id="file-caption" accept="image/*">
    </div>

    <canvas id="canvas-caption" style="margin:0 auto 24px; display:none;"></canvas>

    <div class="loading-bar" id="img2txt-loading">
      <div class="loading-label">
        <span>Downloading captioning model<span class="loading-dots"></span></span>
        <span id="img2txt-loading-pct"></span>
      </div>
      <div class="loading-track"><div class="loading-fill" id="img2txt-loading-fill"></div></div>
      <div class="loading-steps" id="img2txt-loading-status"></div>
    </div>

    <div id="img2txt-results" class="results hidden">
      <div class="output-block mono" id="img2txt-caption">
        Caption will appear here...
      </div>
    </div>

    <div id="img2txt-error" class="results hidden">
      <div class="output-block mono" style="border-left-color:var(--red); color:var(--red);" id="img2txt-error-text"></div>
    </div>
  </section>

  <!-- Footer -->
  <footer>
    <p class="mono">Image-to-Image and Image-to-Text run on your device. Text-to-Image uses the HuggingFace Inference API.</p>
    <p>A workshop tool by Artificial Nouveau</p>
  </footer>

  <!-- Scripts -->
  <script src="js/app.js"></script>

  <!-- Module script for Transformers.js (Image-to-Text) -->
  <script type="module">
    window._loadCaptionPipeline = async function(progressCallback) {
      const { pipeline } = await import('https://cdn.jsdelivr.net/npm/@xenova/transformers@2.17.2');
      const captioner = await pipeline('image-to-text', 'Xenova/vit-gpt2-image-captioning', {
        progress_callback: progressCallback
      });
      return captioner;
    };
  </script>
</body>
</html>
